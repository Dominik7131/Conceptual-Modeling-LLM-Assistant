# Architecture

TODO: C4 model picture

## Main components
- frontend
- LLM backend
- LLM assistant server


## Dataspecer integration
- the frontend is integrated with the [Dataspecer tool](https://github.com/mff-uk/dataspecer)
    - a conceptual model can be imported from this tool or exported into this tool by inserting the corresponding model ID


## LLM assistant workflow
- when the user creates his domain model on the frontend he can call our LLM assistant throught the UI for some help
- when our assistant is called a REST-API request is sent to our [Flask](https://flask.palletsprojects.com/en/3.0.x/) server
    - this server is run by our `server.py` script
    - it contains an instance of the `LLMAssistant` class from the `utils/llm_assistant.py` script
    - all endpoints of this server are [documented here](https://github.com/Dominik7131/Conceptual-Modeling-LLM-Assistant/blob/master/docs/api-endpoints.md)

- then the corresponding method on the instance of the `LLMAssistant` class is called
- now the main goal is to find corresponding prompt in form of a template then fill in all the prompt symbols and then send this prompt to the LLM


### Prompt processing
- `LLMAssistant` uses instance of the `PromptManager` class from `utils/prompt_manager.py` to find the corresponding prompt template
- then the class `Multireplacer` from `utils/multireplacer.py` replaces the prompt symbols defined inside `definitions/prompt_symbols.py` with the corresponding parameters from the user
- when the domain description is being filled in it can be firstly filtered by either our semantic or syntactic algorithm
    - first the domain description is splited into chunks by the class `TextSplitter` from `utils/text_splitter.py`

    - the semantic algorithm uses for filtering instance of the class `SemanticTextFilterer` from `text-filterer/semantic_text_filterer.py`
        - it uses a language model from [SentenceTransformers](https://sbert.net/) library
    - the syntactic algorithm uses for filtering instance of the class `SyntacticTextFilterer` from `text-filterer/syntactic_text_filterer.py`
        - it uses a language model from [MorphoDiTa](https://pypi.org/project/ufal.morphodita/) library

- when all symbols in the prompt are filled in the prompt is ready to be sent to the LLM backend
    - to run the LLM backend we use [llama.cpp server](https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md)
    - the LLM backend is run by the script `llm_backend.sh`


### LLM output processing
- the `LLMAssistant` uses the instance of the class `OutputGenerator` from `utils/output_generator.py` to send the prompt to the LLM backend and to parse the output
- if the outputed object contains original text then an instance of the class `OriginalTextFinder` from `utils/original_text_finder.py` is used to find it's original text indexes to possibly highlight to the user this object in the domain description
    - the class `OriginalTextFinder` is able to recover from some minor misstakes which is tested inside the script `tests/find_original_text_indexes.py`
- also we are automatically converting any generated name from any convention into the standard convention inside the script `utils/convention_convertor.py`
    - this conversion is tested inside the `tests/convention_convertor.py` script

- as soon as some object is generated by the LLM and checked by our scripts it is sent back to the user on the frontend for a faster response time

- if the user wants to highlight the part of his domain description that he has already covered with his domain model then the instance of the class `OriginalTextMerger` is used to merge all original text indexes
    - correct merging is tested inside `tests/merge_original_text_indexes.py`