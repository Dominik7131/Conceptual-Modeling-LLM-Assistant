<!DOCTYPE html>
<html>
<head>
<title>thesis-notes-cz.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<h1 id="thesis-notes">Thesis notes</h1>
<ul>
<li>text diplomky pak bude v angličtině, poznámky zatím píšu v češtině</li>
</ul>
<br />
<h2 id="featury-llm-asistenta">Featury LLM asistenta</h2>
<ul>
<li>obecné pravidlo, kterým se řídíme: LLM se může kdykoliv splést, proto finální rozhodnutí je vždy na uživateli, jestli chce daný návrh příjmout</li>
</ul>
<br />
<ol>
<li>navrhování prvků konceptuálního modelu
<ul>
<li>
<p>vyjmenování jednotlivých úloh:</p>
<ul>
<li>návrh entit</li>
<li>návrh atributů pro zadanou entitu</li>
<li>návrh vztahů pro zadanou zdrojovou, nebo cílovou entitu</li>
<li>návrh vztahů pro zadanou zdrojovou a cílovou entitu</li>
</ul>
</li>
<li>
<p>poznámka: dále by bylo možné i navrhovat generalizace (možná to v rámci diplomky přidám)</p>
</li>
<li>
<p>TODO: podrobněji popsat všechny jednotlivé úlohy a nejspíš uvést konkrétní příklady</p>
</li>
<li>
<p>ke každému návrhu snaha ukázat kontext kolem něj, aby se uživatel mohl rychle rozhodnout, jestli chce příslušný návrh použít, nebo ne</p>
<ul>
<li>například pokud je zadán popis domény, tak lze si každý návrh nechat zobrazit v popisu domény</li>
<li>to slouží k tomu, aby se s aplikací rychleji a lépe pracovalo</li>
</ul>
</li>
</ul>
</li>
</ol>
<br />
<ol start="2">
<li>sumarizace označené části konceptuálního modelu
<ul>
<li>
<p>varianty:</p>
<ul>
<li>nestrukturovaná sumarizace v plain textu</li>
<li>strukturovaná sumarizace v popiscích</li>
</ul>
</li>
<li>
<p>TODO: popsat, co umožníme uživateli nastavovat za parametry</p>
</li>
</ul>
</li>
</ol>
<br />
<ol start="3">
<li>zvýraznění toho, co má uživatel již namodelováno
<ul>
<li>zde lze rozebrat:
<ul>
<li>
<p>jakým způsobem automaticky hledám vygenerované původní texty v popisu domény</p>
</li>
<li>
<p>zmínit, jak funguje sjednocování původních textů, když uživatel chce v popisu domény zvýraznit původní texty pro více prvků</p>
</li>
<li>
<p>na jaké hlavní problémy jsem narazil a proč některé zvýrazněné prvky v popisu domény ve skutečnosti nemusí být namodelovány a naopak</p>
<ul>
<li>
<p>například dejme tomu, že v popisu domény je uvedena tato věta: &quot;Student je osobou, která...&quot;</p>
<ul>
<li>pokud je tato věta obsažena v původním textu pro studenta, tak ve chvíli, kdy uživateli zvýrazníme tento původní text v popisu domény, tak to bude vypadat, jako kdyby uživatel už měl namodelované dvě entity: student a osoba</li>
<li>možné řešení: u entit negenerovat původní text, jenom v popisu domény zvýraznit název příslušné entity</li>
</ul>
</li>
<li>
<p>příklad problému u atributů: &quot;Totožnost osoby se prokazuje buď občankou, nebo řidičákem.&quot;</p>
<ul>
<li>dává smysl, že atribut &quot;občanka&quot; u entity &quot;osoba&quot; jako původní text bude mít celou tu větu z příkladu</li>
<li>opět problém, že při zvýraznění tohoto původního textu v popisu domény to bude vypadat, jako kdyby uživatel měl namodelované dva prvky: občanku a řidičák
<ul>
<li>to znamená, že ten původní text pro atribut &quot;občanka&quot; by měl vypadat spíš takto: &quot;Totožnost osoby se prokazuje buď občankou&quot;
<ul>
<li>zde možná malý problém, že z toho původního textu nemusí být zřejmá kardinalita pro občanku</li>
<li>problém: je těžké přesně vysvětlit LLM, jak má vypadat vygenerovaný původní text
<ul>
<li>asi nejlepší by bylo dát do promptu hromadu příkladů původních textů a odpovídajících prvků, aby LLM z toho odvodil, jak ty původní texty má generovat</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>jako future work by se dal zkusit obrácený přístup:</p>
<ul>
<li>vstup LLM: popis domény a uživatelův konceptuální model</li>
<li>výstup LLM: části popisu domény, které nejsou namodelovány v uživatelově konceptuálním modelu
<ul>
<li>plus případně k tomu nějaká úroveň jistoty od 1 do 10, jak moc si LLM myslí, že ten text není namodelován v uživatelově konceptuálním modelu</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<ul>
<li>TODO: ke každému bodu by to chtělo popis a nejlépe názornou ukázku na obrázcích</li>
</ul>
<br />
<h2 id="data">Data</h2>
<ul>
<li>
<p>TODO: vyjmenovat a popsat, s jakými popisy domén pracujeme</p>
<ul>
<li>charakterizovat ty 3 varianty popisů domén</li>
</ul>
</li>
<li>
<p>TODO: hodilo by se sem dát tabulku s počtem entit, atributů a vztahů pro každý popis domény, aby bylo vidět, s jak velkými texty pracujeme</p>
</li>
<li>
<p>máme i anotované popisy domény a zdůvodnit proč</p>
<ul>
<li>zde každý prvek očekávaného modelu má k sobě původní text, ze kterého vyplývá</li>
<li>hlavní důvody:
<ul>
<li>lze automaticky otestovat filtrování popisu domény</li>
<li>lze do promptu dát konkrétní příklady, jakým způsobem má LLM generovat původní texty pro vygenerované prvky</li>
<li>původní texty zachycují sémantiku každého prvku, tudíž při manuálním zhodnocení lze snáze porovnat, jestli vygenerovaný prvek odpovídá příslušnému očekávanému prvku</li>
</ul>
</li>
</ul>
</li>
<li>
<p>TODO: vysvětlit naše předpoklady: popis domény, který nejprve nějaký analytik určitým způsobem upraví</p>
<ul>
<li>ale jinak dokážeme pracovat s libovolným popisem domény, jenom kvalita výstupu bude možná o trochu horší</li>
</ul>
</li>
<li>
<p>jeden popis domény s očekávaným modelem používáme pro vkládání konkrétních příkladů do promptů</p>
<ul>
<li>ten popis domény je co nejobecnější, aby mu každý LLM co nejvíce rozuměl</li>
<li>důvod: doufáme, že tím pádem i těm našim příkladům LLM co nejlépe porozumí a dokáže z nich pochopit, co po něm chceme</li>
</ul>
</li>
</ul>
<br />
<h2 id="jak%C3%A9-llm-pou%C5%BE%C3%ADv%C3%A1me">Jaké LLM používáme</h2>
<ul>
<li>
<p>poznámka: bude dobré někde na začátku upozornit na to, že pracujeme se středně velkým LLM</p>
<ul>
<li>proto je možné, že narážíme na některé problémy, které by s větším LLM nenastaly</li>
</ul>
</li>
<li>
<p>pro odladění promptů používáme <a href="https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF">tento</a> LLM</p>
<ul>
<li>konkrétně používáme variantu <code>mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf</code>
<ul>
<li>každý parametr je místo 16 bitů zakódován do přibližně 5 bitů</li>
</ul>
</li>
</ul>
</li>
<li>
<p>TODO: pak zmínit, jaké další LLM jsme použili pro experimenty a zdůvodnit, proč jsme je vybrali</p>
</li>
</ul>
<br />
<h2 id="prompty">Prompty</h2>
<h3 id="obecn%C3%A1-pravidla-kter%C3%BDmi-se-%C5%99%C3%ADd%C3%ADme-p%C5%99i-promptov%C3%A1n%C3%AD">Obecná pravidla, kterými se řídíme při promptování</h3>
<ul>
<li>
<p>prompty píšeme v angličtině, protože většina LLM při trénování viděly nejvíce textů v tomto jazyce, tudíž anglickému vstupu a výstupu nejlépe rozumí</p>
</li>
<li>
<p>nestrukturovaný vstup dáváme v plain textu a strukturovaný vstup dáváme v JSON formátu</p>
<ul>
<li>důvod: typicky největší množina trénovacích dat pro LLM pochází z internetu
<ul>
<li>zde je nejvíce rozšířený:
<ul>
<li>plain text pro nestrukturované předávání informací</li>
<li>nejspíš JSON pro strukturované předávání informací</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>prompty sami předpřipravujeme jako šablony, do kterých se později vyplní jednotlivé položky na základě uživatelova vstupu</p>
<ul>
<li>nechceme uživateli dát možnost napsat si vlastní prompt, protože na formátu a obsahu promptu hodně záleží</li>
<li>hlavní důvody, proč si prompty předpřipravit jako šablony:
<ul>
<li>potřebujeme co nejvíce konzistentních podmínek pro otestování, jestli LLM dává rozumné výstupy pro různou množinu vstupů</li>
<li>na konkrétních slovech v promptu hodně záleží
<ul>
<li>v nejhorším případě se může stát, že LLM vůbec promptu nebude rozumět a uživatel bude zbytečně čekat na nesmyslný výstup, který pak nevyužije</li>
<li>existuje celá řada tipů, jak psát prompty viz například <a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions">zde</a>
<ul>
<li>poznámka: zde lze shrnout pár nejlepších praktik pro psaní promptů
<ul>
<li>například konzistentní používání pojmů: pokud v jedné větě mluvíme o entitách, tak v další větě se na ně odkazovat jako na třídy (nebo jiným podobným výrazem) může způsobit zmatení</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>výstup LLM potřebujeme umět automaticky naparsovat
<ul>
<li>tedy minimálně musíme předpřipravit část promptu, která specifikuje formát výstupu</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="struktura-promptu">Struktura promptu</h3>
<p><a href="https://github.com/Dominik7131/Conceptual-Modeling-LLM-Assistant/tree/master/prompts">Zde</a> je odkaz na seznam všech našich promptů. Typicky se naše prompty skládají z těchto částí:</p>
<ol>
<li>hlavní řídící instrukce</li>
<li>specifikace konkrétního postupu</li>
<li>příklad výstupního JSON formátu</li>
<li>konkrétní příklad vstupu a výstupu</li>
<li>vstupní data</li>
</ol>
<h4 id="1-hlavn%C3%AD-%C5%99%C3%ADd%C3%ADc%C3%AD-instrukce">1) Hlavní řídící instrukce</h4>
<ul>
<li>
<p>zde přesně specifikujeme, co chceme vykonat</p>
<ul>
<li>například: &quot;Generate attributes for given entity: {source_entity}&quot;
<ul>
<li>{source_entity} je značka, za kterou se doplní ta entita, pro kterou uživatel chce vygenerovat atributy
<ul>
<li>seznam všech značek je <a href="https://github.com/Dominik7131/Conceptual-Modeling-LLM-Assistant/blob/e2b741a5bba4e1776b45c62111f797d4d69767a0/text_utility.py#L53">zde</a>
<ul>
<li>TODO: zdokumentovat všechny značky na GitHubu</li>
</ul>
</li>
</ul>
</li>
<li>chceme ten problém pojmenovat podle toho, jak se mu nejčastěji říká na internetu, protože odtud je typicky většina trénovacích dat, aby LLM jen podle názvu daného problému už s co největší pravděpodobností věděl, co se po něm chce
<ul>
<li>poznámka: jako future work by bylo zajímavé vyzkoušet, jakým způsobem se u vztahů změní kvalita výstupu, když místo pojmu &quot;relationship&quot; použijeme pojem &quot;association&quot;
<ul>
<li>moje hypotéza je, že to kvalitu výstupu téměř neovlivní, protože se jedná o sémanticky podobné pojmy a tudíž je LLM bude chápat podobně</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>chceme toto udělat úplně na začátku, protože podle několika zdrojů, LLM jsou typicky natrénovány tak, aby největší prioritu přiřadily textu na začátku promptu</li>
</ul>
</li>
<li>
<p>pokud chceme, aby se LLM čistě držel popisu domény, pak první slova promptu obsahují &quot;Solely based on the given context&quot;</p>
<ul>
<li>poznámka: tady by se možná hodilo psát &quot;Solely based on the given <strong>domain description</strong>&quot;</li>
<li>pokud chceme, aby se LLM čistě držel svých natrénovaných parametrů, tak prostě nic neřekneme</li>
</ul>
</li>
</ul>
<h4 id="2-specifikace-konkr%C3%A9tn%C3%ADho-postupu">2) Specifikace konkrétního postupu</h4>
<ul>
<li>
<p>poznámka: zde lze dát nějaké ukázky článků o tom, že chain of thought a tree of thoughts v některých případech o hodně zlepšují kvalitu výstupu, především v úlohách, které vyžadují řešení krok po kroku</p>
<ul>
<li>na takové články jsou odkazy například <a href="https://www.mercity.ai/blog-post/guide-to-chain-of-thought-prompting#cot-vs-other-methods">zde</a></li>
<li>poznámka: zde lze vyjmenovat všechny možné chain of thoughts varianty a zamyslet se nad tím, která je pro nás nejvhodnější
<ul>
<li>například: Least-to-Most Prompting, Auto-CoT</li>
</ul>
</li>
</ul>
</li>
<li>
<p>úloha generování návrhů pro daný prvek konceptuálního modelu se typicky skládá ze dvou částí (neboli jakým způsobem by tu úlohu řešil člověk):</p>
<ol>
<li>nalézt kontext pro daný prvek
<ul>
<li>poznámka: Retrieval augmented generation (RAG) má za úkol tuto úlohu zjednodušit</li>
</ul>
</li>
<li>z kontextu vyextrahovat konkrétní informace, jako je například název daného prvku</li>
</ol>
<ul>
<li>například pro vygenerování atributů pro zadanou entitu nejdříve potřebujeme najít v popisu domény místa, kde se příslušná entita nachází a potom z těchto míst vyextrahovat případné atributy</li>
</ul>
</li>
<li>
<p>proto v našich promptech jako první chceme vygenerovat pro daný návrh původní text (to je ten zmíněný kontext pro daný prvek) a potom až chceme nechat vygenerovat například název konkrétního návrhu</p>
<ul>
<li>
<p>cílem je přinutit LLM, aby nejprve vykonal ten zmíněný první krok a pak až druhý</p>
<ul>
<li>obava je taková, že pokud LLM jako první vygeneruje například název atributu, tak:
<ul>
<li>jednak to může znamenat, že dělá ty oba zmíněné kroky najednou, což možná může zhoršit kvalitu výstupu podobně, jako když se LLM snaží vyřešit matematickou slovní úlohu najednou a kvůli tomu vydá špatný výsledek (je to jen hypotéza, bylo by to potřeba pořádně otestovat)</li>
<li>druhak to může znamenat, že LLM bude původní text generovat na základě toho předem vygenerovaného názvu atributu, což může být problém, pokud je název atributu špatně (opět by to bylo potřeba pořádně otestovat)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>podle našeho menšího experimentu to vypadá, že lze dále zlepšit kvalitu výstupu tím, že si necháme původní text a název daného prvku nejdřív vypsat v plain textu a až potom si to necháme vypsat v JSON formátu</p>
<ul>
<li>tento přístup označujeme jako &quot;chain of thoughts&quot;</li>
</ul>
</li>
<li>
<p>poznámka: z nějakého důvodu toto pro generování entit nefunguje, ale třeba to jenom nefunguje pro náš LLM</p>
</li>
<li>
<p>poznámka: tento přístup používáme pro generování atributů a vztahů</p>
</li>
</ul>
</li>
<li>
<p>poznámka: zde bych potom mohl něco napsat o tree of thoughts až to vyzkouším</p>
</li>
</ul>
<h4 id="3-p%C5%99%C3%ADklad-json-form%C3%A1tu">3) Příklad JSON formátu</h4>
<ul>
<li>
<p>hlavní důvod: slouží pro specifikování JSON formátu, který chceme na výstupu, abychom ho pak mohli automaticky naparsovat</p>
</li>
<li>
<p>podobně jako v předchozím bodě i zde se při specifikování formátu výstupu držíme toho, že jako první uvedeme položku pro původní text, aby LLM jako první hledal původní text pro daný prvek</p>
</li>
</ul>
<h4 id="4-konkr%C3%A9tn%C3%AD-p%C5%99%C3%ADklad-vstupu-a-v%C3%BDstupu">4) Konkrétní příklad vstupu a výstupu</h4>
<ul>
<li>
<p>provádíme tzv. few-shot prompting</p>
</li>
<li>
<p>hlavní důvody použití:</p>
<ol>
<li>
<p>možné zvýšení kvality výstupu díky tomu, že LLM lépe promptu porozumí</p>
<ul>
<li>neboli v promptu nejdříve slovy popíšeme a pak názorně ukážeme, co chceme</li>
<li>pokud LLM nerozumí tomu, co například znamená &quot;generate relationships&quot;, tak to aspoň může zkusit pochopit z těchto konkrétních příkladů
<ul>
<li>pokud LLM slovnímu zadání rozumí, tak na tom příkladu se aspoň ujistí, že ví, co má generovat</li>
</ul>
</li>
<li>poznámka: zde lze uvést některé články ukazující, že few-shot prompting v některých úlohách o hodně zlepšuje kvalitu výstupu</li>
</ul>
</li>
<li>
<p>zlepšení názvů návrhů a obecně i ostatních položek</p>
<ul>
<li>
<p>bez konkrétního příklad není jasné, v jakém formátu mají být například názvy jednotlivých atributů</p>
<ul>
<li>například jestli slova mají být oddělena podtržítkem (snake_case), velkým písmenem (camelCase, PascalCase), nebo standardně mezerou</li>
<li>ale pro jistotu stejně kontrolujeme, že každá položka výstupu je ve standardním formátu
<ul>
<li>důvod: LLM se kdykoliv může splést, nebo něco špatně pochopit</li>
</ul>
</li>
</ul>
</li>
<li>
<p>kromě změny formátu také může dojít ke změně toho, jakým způsobem názvy vypadají</p>
<ul>
<li>například bez few-shot promptování se může stát, že každý atribut bude vypadat takto: &quot;has name&quot;, &quot;has ID&quot;, &quot;has email&quot;, atd.</li>
<li>ale když uvedeme example, tak ty názvy atributů pak budou vypadat takto: &quot;name&quot;, &quot;ID&quot;, &quot;email&quot;, což je více typický název pro atributy</li>
</ul>
</li>
</ul>
</li>
<li>
<p>pro user experience, abychom uživateli mohli rychleji začít ukazovat výstup</p>
<ul>
<li>examplem názorně ukážeme, jakým způsobem má LLM generovat výstup</li>
<li>toto je hlavně potřeba u našeho chain of thoughts přístupu, aby LLM ke každému návrhu rovnou vygeneroval i příslušný JSON objekt a nestalo se, že všechny JSON objekty budou až úplně na konci výstupu</li>
<li>poznámka: toto je hlavně potřeba pro méně kvalitní LLM, ale například z mé zkušenosti ChatGPT-3.5 pochopil hned podle specifikování konkrétního postupu, jakým způsobem má výstup vygenerovat a konkrétní příklad nebyl potřeba</li>
</ul>
</li>
</ol>
</li>
<li>
<p>otázkou je, jaký je ideální počet příkladů</p>
<ul>
<li>nedostatek příkladů: riziko, že LLM nepochopí, co se po něm chce</li>
<li>moc příkladů:
<ul>
<li>zbytečně plýtváme context window size</li>
<li>možná &quot;přetrénování&quot; především u slabších LLM v tom smyslu, že LLM vidí tolik příkladu z ukázkového popisu domény, že pak začne z tohoto popisu domény generovat návrhy a bude ignorovat popis domény zadaný od uživatele</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="5-vstupn%C3%AD-data">5) Vstupní data</h4>
<ol>
<li>
<p>pro některé prompty zde dáme konceptuální model v JSON formátu</p>
<ul>
<li>future work: hodily by se experimenty s dalšími formáty, jestli by to vylepšilo kvalitu výstupu
<ul>
<li>hypotéza: kvalitu výstupu to znatelně neovlivní, dokud použijeme formát, který příslušný LLM viděl hodněkrát v trénovacích datech</li>
</ul>
</li>
</ul>
</li>
<li>
<p>pro některé prompty zde dáme popis domény v plain textu</p>
<ul>
<li>provádíme filtrování popisu domény
<ul>
<li>to znamená, že se snažíme do promptu dát pouze ty informace z popisu domény, které LLM potřebuje k vygenerování správného výstupu
<ul>
<li>například pokud uživatel chce atributy pro entitu &quot;animal&quot;, tak do promptu chceme dát pouze části textu hovořící zvířatech</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ol>
<h2 id="retrieval-augmented-generation-rag">Retrieval augmented generation (RAG)</h2>
<ul>
<li>
<p>poznámky:</p>
<ul>
<li>hodila by se nějaká definice</li>
<li>hodilo by se z obecného hlediska popsat, jak to funguje a ukázat nějaký obrázek</li>
<li>lze vyjmenovat několik základních RAG přístupů
<ul>
<li>hromadu různých přístupů popisuje tento <a href="https://arxiv.org/pdf/2312.10997">článek</a></li>
</ul>
</li>
<li>dalo by se tady pak uvést i základní RAG pitfally z článku <a href="https://arxiv.org/abs/2401.05856">zde</a> a jakým způsobem se jim vyhýbáme</li>
</ul>
</li>
<li>
<p>RAG používáme ve formě filtrování popisu domény při generování atributů a vztahů</p>
<ul>
<li>při generování entit nefiltrujeme popis domény, protože nemáme podle čeho filtrovat</li>
</ul>
</li>
<li>
<p>hlavní důvody proč to děláme:</p>
<ol>
<li>
<p>když z popisu domény dáme pryč neužitečné informace, tak tím má LLM menší prostor pro dělání chyb, čímž potenciálně zlepšíme recall a precision při generování atributů a vztahů</p>
</li>
<li>
<p>výsledný prompt ve výsledku bude kratší, což znamená, že ho LLM rychleji zpracuje a uživatel o trochu rychleji uvidí výstup</p>
<ul>
<li>pokud bychom používali placený LLM, tak zde se platí za každý token a tímto bychom snížili výslednou částku</li>
<li>poznámka: zde bychom třeba mohli zmínit, kolik času llama.cpp zpracovává prompt například s x tokeny
<ul>
<li>x můžeme odhadnout zprůměrováním velikostí našich testovacích popisů domén</li>
<li>možná ale ta informace bude k ničemu, protože aby ta naše aplikace byla použitelná pro více uživatelů, tak bychom stejně potřebovali více grafických karet a pravděpodobně i lepší LLM</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
<li>
<p>varianty filtrování:</p>
<ul>
<li>žádné (slouží pro porovnání s ostatními přístupy)</li>
<li>syntaktické</li>
<li>sémantické</li>
</ul>
</li>
<li>
<p>záleží na tom, na jak velké části budeme popis domény rozdělovat</p>
<ul>
<li>když části budou příliš velké, tak riziko, že v promptu zbytečně budou nepotřebné části textu</li>
<li>když naopak části budou příliš malé, tak riziko, že nebudou obsahovat dostatek kontextu k tomu, aby filtrovací algoritmy dokázaly správně posoudit, jestli tu část textu mají ponechat, nebo odstranit</li>
</ul>
</li>
<li>
<p>popis domény dělíme na jednotlivé věty</p>
<ul>
<li>plus pro přidání kontextu každá věta obsahuje metadata, kde například může být odkaz na nějakou jinou větu
<ul>
<li>například text za odrážkou k sobě vždy do metadat dostane nadpis nad odrážkami (neboli nejbližší větu, před kterou není odrážka)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>poznámka: zde lze navázat na tu zmiňovanou ztrátu kontextu u příliš malých částí textu a mít tu podkapitolu o řešení problému se zájmeny</p>
<ul>
<li>toto zlepší recall i precision pro syntaktické i sémantické filtrování</li>
<li>zjednodušená varianta: pokud věta začíná zájmenem, tak té větě dáme do metadat odkaz na předchozí větu, protože typicky předchozí věta obsahuje tu entitu, na kterou se to zájmeno odkazuje</li>
<li>nejspíš lepší varianta: mít jazykový model, který na začátku dostane celý popis domény a pro každé zájmeno nám řekne, na co se odkazuje
<ul>
<li>potom tuto informaci nějak zakomponujeme do metadat příslušné věty</li>
<li>tomuto nahrazování zájmen se říká tzv. coreference resolution</li>
<li>poznámka: zde by se hodil rozbor modelů, které dělají coreference resolution a zdůvodnit náš výběr modelu</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="syntaktick%C3%A9-filtrov%C3%A1n%C3%AD">Syntaktické filtrování</h3>
<ul>
<li>
<p>motivace: z předpokladů o našich datech víme, že v popisu domény jednotlivé prvky nemají obsahovat synonyma</p>
<ul>
<li>proto kontext pro příslušnou entitu lze hledat syntakticky na základě názvu této entity</li>
</ul>
</li>
<li>
<p>základní princip: uživatel nám zadá entitu, pro kterou chce vygenerovat atributy, nebo vztahy</p>
<ul>
<li>název této entity převedeme do základních tvarů na tzv. lemmata</li>
<li>následně odstraníme ty věty z popisu domény, které neobsahují tato lemmata
<ul>
<li>technická poznámka: pro lemmatizaci používáme MorphoDiTu
<ul>
<li>pro zajištění lepší konzistence děláme to, že převádíme vždy slova jednotlivě na lemmata
<ul>
<li>protože když převádíme například celé věty na lemmata, tak se může stát, že slovo v jednom kontextu má určité lemma, ale to samé slovo v jiném kontextu může mít jiné lemma, což způsobuje problémy</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="s%C3%A9mantick%C3%A9-filtrov%C3%A1n%C3%AD">Sémantické filtrování</h3>
<ul>
<li>
<p>základní princip: uživatel nám zadá entitu, pro kterou chce vygenerovat atributy, nebo vztahy</p>
<ul>
<li>název této entity jazykový model převede do vektorového prostoru</li>
<li>následně odstraníme ty věty z popisu domény, které ve vektorovém prostoru si nejsou podobné příslušnému názvu entity
<ul>
<li>&quot;nejsou podobné&quot; záleží na tom, jak nastavíme hranici podobnosti</li>
<li>jazykový model vrací podobnost například v rozmezí -1 až 1
<ul>
<li>je na nás, jestli například odstraníme každou větu se skórem &lt; 0.5</li>
<li>problém: skóre jsou relativní
<ul>
<li>to znamená, že v jednom případě má nejpodobnější věta skóre například 0.9 a jindy má nejpodobnější věta skóre například 0.3, proto nastavit pevnou hranici nedává smysl</li>
<li>přijde mi, že poměrně dobře funguje, když stanovím interval, jak daleko může být skóre od maximálního skóre
<ul>
<li>například pokud je tento interval 0.2, tak pokud je maximální skóre = 0.9, tak zahodíme všechny věty, které mají skóre &lt; 0.9 - 0.2 = 0.7 a pokud je maximální skóre = 0.3, tak zahodíme všechny věty, které mají skóre &lt; 0.3 - 0.2 = 0.1</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>poznámka: zde bych mohl klasifikovat jazykové modely pro převod textu do vektorové reprezentace + zdůvodnění toho, jaké modely používáme</p>
<ul>
<li>například symetrické vs. asymetrické (<a href="https://www.sbert.net/examples/applications/semantic-search/README.html">odkaz</a>)</li>
<li>další typy modelů <a href="https://www.sbert.net/docs/pretrained_models.html">zde</a></li>
<li>poznámka: zde je výhoda, že ty modely můžeme automaticky otestovat, takže klidně lze zkusit více modelů
<ul>
<li>jenom nevýhoda, že každému modelu vyhovuje jinak nastavená hranice podobnosti, takže potřeba udělat například for cyklus, který pro každý model vyzkouší několik parametrů</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="parametry-llm-pro-generov%C3%A1n%C3%AD-v%C3%BDstupu">Parametry LLM pro generování výstupu</h2>
<ul>
<li>
<p>temperature = 0</p>
<ul>
<li>TODO: vysvětlit co je to temperature</li>
<li>rozsah temperature je od 0 do 2</li>
<li>vysvětlit proč temperature nastavujeme na 0
<ul>
<li>zkusím to podpořit nějakými zdroji</li>
</ul>
</li>
<li>pro sumarizaci necháváme uživatele si nastavit temperature podle sebe
<ul>
<li>TODO: proč to děláme</li>
</ul>
</li>
</ul>
</li>
<li>
<p>poznámka: zde bych mohl vyjmenovat další hlavní parametry a jakým způsobem ovlivňují výstup</p>
<ul>
<li>například repetition penalty</li>
</ul>
</li>
<li>
<p>chtěl bych nastavit parametry tak, aby se po každé uživateli objevil jiný výstup</p>
<ul>
<li>tedy když s výstupem nebude spokojený, tak si nechá výstup vygenerovat znovu a dostane jiný výsledek</li>
</ul>
</li>
<li>
<p><a href="https://platform.openai.com/docs/api-reference/chat/create">zde</a> je odkaz na API, které používám</p>
<ul>
<li>lze zde vyčíst výchozí hodnoty parametrů</li>
</ul>
</li>
</ul>
<br />
<h2 id="llm-asistent-pipelina">LLM asistent pipelina</h2>
<ul>
<li>TODO: tohle by se pro přehled hodilo někde na začátku, aby si čtenář hned udělal obrázek o tom, jak funguje naše aplikace
<ul>
<li>možná by se hodilo strukturovat diplomku takovým způsobem, abychom postupně popisovali jednotlivé krabičky</li>
<li>možná lze obrázek pipeliny ukázat už v úvodu diplomky a popsat na tom, čím se postupně v textu budeme zabývat</li>
</ul>
</li>
</ul>
<p>Obrázek ze zadání výzkumného projektu:</p>
<img src="LLM-assistant-pipeline.jpg" alt="drawing" width="600"/>
<ul>
<li>
<p>chtělo by to:</p>
<ul>
<li>vyznačit, kde dělám filtrování popisu domény</li>
<li>naznačit variantnost (semantic/syntactic, CoT/non-CoT)</li>
</ul>
</li>
<li>
<p>TODO: lze se inspirovat architekturou z &quot;Automated Domain Modeling with Large Language Models: A Comparative Study&quot;</p>
</li>
</ul>
<img src="architecture-simplified.png" alt="drawing" width="600"/>
<h2 id="ostatn%C3%AD-pozn%C3%A1mky-kter%C3%A9-je-pak-pot%C5%99eba-n%C4%9Bkam-um%C3%ADstit">Ostatní poznámky, které je pak potřeba někam umístit</h2>
<h3 id="extrakce-v%C3%BDstupu-z-llm">Extrakce výstupu z LLM</h3>
<ul>
<li>nefunguje moc dobře si nechat nejdřív vygenerovat celý výstup a ten pak naparsovat
<ul>
<li>hlavní problémy:
<ol>
<li>
<p>z praktických zkušeností víme, že LLM typicky výstup nějak okomentuje</p>
<ul>
<li>i když do promptu dáme instrukci, aby nic nekomentoval, tak toto občas bývá ignorováno (záleží na konkrétním LLM)</li>
<li>tudíž ve výsledku by se výstup nedal automaticky naparsovat, uživatel by zbytečně čekal než se něco vygeneruje a pak by nic nedostal</li>
</ul>
</li>
<li>
<p>pro lepší user experience se hodí parsovat výstup LLM během generování</p>
<ul>
<li>například pokud se první položka výstupu vygeneruje za 1 vteřinu a celý výstup komplet se vygeneruje za 10 vteřin, tak je mnohem lepší uživateli po 1 vteřině zobrazit první položku, než uživateli 10 vteřin nezobrazovat nic a až potom celý výsledek
<ul>
<li>zde lze poznamenat, že naše konfigurace generuje tokeny přibližně rychlostí 50 tokenů za vteřinu, kde 1 token přibližně odpovídá 3/4 jednoho slova</li>
</ul>
</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<h3 id="%C3%BAprava-v%C3%BDstupu-llm">Úprava výstupu LLM</h3>
<ul>
<li>
<p>odstranění vztahů, které neobsahují v source/target entitě uživatelem zadanou entitu</p>
</li>
<li>
<p>odstranění návrhu, pokud neobsahuje všechny povinné položky</p>
</li>
<li>
<p>situace, kdy generování výstupu automaticky ukončíme</p>
<ul>
<li>když LLM začne generovat jeden symbol pořád dokola</li>
<li>když se LLM rozhodne vygenerovat moc návrhů
<ul>
<li>když uživatel neposkytne popis domény, tak může nastavit maximální počet položek a pak víme, kdy zastavit</li>
<li>ale když uživatel poskytne popis domény, tak je dobré po určitém počtu vygenerovaných návrhů zastavit, protože typicky uživatel nebude chtít zkoumat například 100 vygenerovaných návrhů a typicky moc výstupů znamená, že LLM neví, co se po něm chce</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="logov%C3%A1n%C3%AD-n%C4%9Bkter%C3%BDch-akc%C3%AD-u%C5%BEivatele">Logování některých akcí uživatele</h3>
<ul>
<li>
<p>hlavní důvody:</p>
<ul>
<li>případný fine-tuning nějaké LLM</li>
<li>feedback pro nás, co je potřeba vylepšit</li>
<li>případná data pro zhodnocení kvality výstupů</li>
</ul>
</li>
<li>
<p>poznámka: stručně vyjmenovat, jaká data ukládáme</p>
</li>
<li>
<p>poznámka o tom, proč nelogujeme každou akci uživatele</p>
<ul>
<li>protože například uživatelem nepoužitý návrh může být naprosto v pořádku, jenom zrovna ho uživatel nechtěl použít</li>
<li>protože například uživatel může návrh přidat a pak ho někdy upravit/odstranit
<ul>
<li>to by vyžadovalo, abychom vytvořili skripty, které posloupnost takových akcí detekují a příslušným způsobem upraví uložená data
<ul>
<li>není jasné, z jakého důvodu něco uživatel přidal a pak třeba odstranil</li>
<li>bylo by složité se v uložených datech vyznat</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<br />
<h1 id="vyhodnocen%C3%AD">Vyhodnocení</h1>
<ul>
<li>recall a precision pro automatické testy z filtrování popisu domény ve všech variantách</li>
<li>recall a precision ručního zhodnocení pro vygenerované entity, atributy a vztahy</li>
<li>možná výsledky z nějakých dotazníků, jak se s naší aplikací pracuje
<ul>
<li>například pro zadaný popis domény, jak dobře se s naší aplikací vytvářel konceptuální model</li>
</ul>
</li>
</ul>

</body>
</html>
