\section{Application}

To empirically validate the feasibility of our proposed methodology, we developed a prototype application. This application contains it's frontend and backend. The backend  implements the generators and other services and provides them via an API. Users can use our frontend via web application that communicates with the backend. The web application contains a text area for inserting a domain description and a modeling canvas where the users create the domain models using classical manual modeling features and the suggestion features highlighted by the \textit{magic wand} icon. Our LLM based assistant provides the following services:
\begin{enumerate}
\item suggesting domain elements
\item highlighting already modeled elements
\item summarising domain model \\
\end{enumerate}

\noindent{}TODO: Brief chapter description \\


\subsection{Suggesting domain elements}

When a domain description is provided, the assistant generates suggestions solely based on the provided text. Otherwise, the assistant generates anything it considers relevant. Our assistant is able to suggest classes and for a selected class the assistant provides functionality to suggest it's attributes and associations. And for a selected source class and a selected target class the assistant is able to suggest their associations. Figure \ref{fig:assistant-features} demonstrates some of these features.

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.22]{img/assistant-features.png}
    \caption{\centering Screenshots of the main functionalities of the prototype application}
    \label{fig:assistant-features}
\end{figure}

Screenshot 1 shows the text area for the domain description and action buttons. The highlighted button \textit{Suggest classes} calls the $gen_c$ operator with the inserted domain description. Screenshot 2 shows the list of suggested classes. The user can use the highlighted ``\textit{+}'' buttons to insert suggested classes into the modeling canvas. The other buttons are used for \textit{liking} and \textit{disliking} the corresponding suggestion, for editing before inserting the suggested class into the canvas, and highlighting the suggestion in the original domain description. For the selected class, the user can ask for attribute and association suggestions, i.e. $gen_a$ and $gen_{r1}$ operators for the class and the domain description. Screenshot 3 shows the attribute suggestions. Screenshot 4 shows the association suggestions. For the suggested attributes and association, the tool also shows the original text returned by the operators $gen_a$, $gen_{r1}$ and $gen_{r2}$.


\subsubsection{Context highlighting}

As mentioned, for each suggested attribute and association we highlight it's original text in the domain description. Additionally, for each class we highlight it's name. This requires to match the LLM generated text with the domain description. The matching is easy as long as the generated text syntactically exactly matches some part of the domain description.

However, we encountered some common issues with this approach. For example, sometimes the LLM changes some letters when generating the text. For instance, when domain description contains the word ``\textit{motorized}'' with the ``\textit{z}'' letter the LLM can generate the word ``\textit{motorised}'' with the ``\textit{s}'' letter instead.

To mitigate this issue we implemented the following recovery strategy. When the generated text and the domain description cannot be directly matched we find their longest common substring and we split the original text into the longest common substring and the remaining parts. Subsequently, we try to match these parts without the letter that was in between these parts. For example, the longest common substring of the words ``\textit{motorized}'' and ``\textit{motorised}'' is the word ``\textit{motori}'' and the remaining part is the ``{\textit{ed}}''. Subsequently, we try to find all occurrences of the word ``\textit{motori}'' in the domain description and for each occurrence we highlight the words that end with the remaining part ``\textit{ed}''. In more complex scenarios this recovery strategy can fail however, this issue can be greatly mitigated by using a LLM with a high output quality.


\subsubsection{Single field suggestion}

For the best possible LLM output quality and response time when generating suggestions we try to simplify the prompts as much as possible. This for example means that the generated suggestions of attributes do not contain the attribute description or the attribute data type. For generating these additional fields our assistant can be used as it implements the generators for suggesting descriptions, data types and cardinalities. Furthermore, if the generated domain element name is not suitable it can be re-generated by the generator for names suggestions. \\

\noindent{}TODO: zmínit nastavení formátu pro jména až to budu mít naimplementované \\


\subsubsection{Attributes and associations conversion}

Attributes can be usually modeled as an associations and vice versa. For example, \textit{address} can be an attribute of a class \textit{person} but it also can be an association in between the classes \textit{person} and \textit{address}. Therefore, for each attribute suggestion we provide functionality to convert them to an association and vice versa. We convert an attribute into an association by putting the attribute name into the association target class and we put the attribute name into the association name. As the copied name does not have to fit, the LLM assistant can be used to generate a new name. The opposite conversion works the same but in reverse.


\subsubsection{Duplicate domain elements}
\label{duplicate_domain_elements}

The generated domain element suggestion is not shown to the users if they already modeled the corresponding element. We remove these suggestions on the backend after the LLM generates the output so we do not have to provide the user's domain model in the prompts for generating classes, attributes and associations.

The domain element suggestion is removed if it's name syntactically matches the name of the user's corresponding modeled element. This approach has the following limitations.

First limitation is that if two domain elements have the same name but a different semantics then a relevant domain element is removed. For example, the class ``car'' can have attribute ``year'' referring to the year of the car manufacturing and the same attribute referring to the year of technical inspection. In our experience, this issue is very rare as the LLM usually generates suggestions with explicit names so in this case the first mentioned attribute would probably be name ``year of manufacturing'' and the second attribute would probably be named ``year of technical inspection''. However, if more of these issues occur, removal of duplicate domain elements can be disabled.

Second limitation is that semantically same domain elements with a different name are not removed. Possible solutions are to either use a LLM or some embedding model as in our RAG semantic approach. However, both have significant disadvantages. Solving the mentioned problem with a LLM in a single prompt approach complicates the prompt wording and can reduce the output quality. Using LLM in an iterative prompt approach can significantly increase the delay between the user's request and the suggestions displaying as the LLM has to process more than one prompt. When using some embedding model as mentioned in the section \ref{sec:top_k_search}, one of the challenging tasks is setting the threshold of the decision boundary  between accepting and rejecting the corresponding element. For example, attributes \textit{first name} and \textit{last name} are very close in terms of vector space distance however, they represent a two semantically distinct different attributes. This fact would force us to set very strict threshold that would reject almost any two syntactically different words which is in result almost identical to our naive approach.


\subsection{Highlighting already modeled elements}

We extend the mentioned highlighting of suggested domain elements in the domain description by highlighting all the selected user's modeled elements in the domain description. However, these highlighted parts does not have to always correspond with the parts that user has already modeled as we only instruct the LLM to generate the context for each attribute and association. This means that this context can contain other classes, attributes or associations. On the other hand, not highlighted part of the domain description can also be already covered by the user's domain model for example, if these parts were modeled manually by the user.


\subsection{Summarising domain model}
\label{summarising_domain_model}

For a selected part of the user's conceptual model the assistant is able to generate a summary for each class, attribute and association. We implemented two summary variations: in a plain text and in a form of a bullet points.

The plain text summary generates a paragraph describing the user's selected domain model. The summary style can be changed in the settings. The available styles are ``analytical'', ``educational'' and ``funny story''. When a style is selected the prompt for plain text summary is edited so it instructs the LLM to generate the output in the selected style. The summary in form of bullet points for each element generates a description.

In both cases the LLM does not receive the user's domain description as this led to a many situations where the LLM did not stick to the selected domain elements.


\subsection{LLM parameters}

For each task we set the temperature to the lowest value so the LLM generates only the most probable output as discussed in the section \ref{temperature}. The only exception is the summary generation where the temperature could be set higher to provide more creative outputs.

The other parameters are implicitly set to their default values which are defined in the OpenAI API\footnote{\url{https://platform.openai.com/docs/api-reference/introduction}}.


\subsection{LLM output checking}

When LLM generates any JSON object we always automatically remove those that does not contain all mandatory fields such as the name when generating classes, attributes and associations. When the generated domain element is an association then we also remove the associations that have a source class or a target class that does not match with the user input. Additionally, as mentioned in the section \ref{duplicate_domain_elements} the generated domain element is removed if it is already present in the user's domain model.


\subsection{Saving users data}

For each generated suggestion users can optionally click on the ``like'' button or the ``dislike'' button. The corresponding evaluated suggestion is sent to the backend and saved there with all the parameters that were used to generate this suggestion. To remember these parameters, the frontend remembers for each suggestion the parameters that were used for generation.

We use these user's reactions for two main reasons. First reason is that we can use these data as a feedback. For example, we can automatically detect if some set of parameters repeatedly ended up with too many negative reactions. After that we can analyse the issue and fix it. Second reason is that we can use these data to fine-tuning some LLM to further improve the quality for our specific tasks.

The downside of our approach is that it does not collect many user data. One possible solution is to save each user action such as saving each suggestion that the user added into his domain model. However, this approach has a lot of disadvantages due to it's complexity. For example, if the user adds some suggestion and then later on removes it we need to save this removal action too as it can mean that the original suggestion turned out to be unwanted. This means that the saved data would need to be post-processed to remove these pairs of data. Similar issue arise when user adds some suggestion but then later on edits it. Because of this complexity we decided to use the explicit reactions buttons.


\subsection{Work-flow}

Figure \ref{fig:work-flow} shows the basic flow of generating suggestions on the backend.

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.23]{img/work-flow.jpg}
    \caption{\centering Schema of the flow of processing the textual domain description}
    \label{fig:work-flow}
\end{figure}

The modeling process typically starts by providing the domain description. Then the domain model is step by step created mainly by using suggestions of classes, attributes and associations from the assistant. Suggestions of attributes and associations are filtered by the selected retrieval-augmentation generation method. Then the prompt engineering techniques are applied and the final prompt is constructed and sent to LLM. Finally, the output from the LLM is parsed and the suggested model elements are sent back to the frontend.