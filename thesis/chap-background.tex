\chapter{Background}

\section{Large Language Models}

LLMs have made a significant breakthrough in the field of natural language processing. These models demonstrated an unprecedented ability to understand and generate human-like text, enabling a wide range of applications across various domains.


\subsection{Architecture}

The foundation of LLMs is the Transformer architecture, introduced by \citet{Vaswani2017}. This architecture relies on self-attention mechanism that allow the model to weigh the importance of different words in a sentence when making predictions. Unlike previous recurrent networks (RNNs) or long short-term memory networks (LSTMs), Transformers can process input data in parallel, significantly improving training efficiency and performance.


\subsection{Parameters}

LLMs are characterized by their large number of parameters, often ranging from hundreds of millions to hundreds of billions. This scaling helps them to learn nuanced patterns and relationships in the data.


\subsection{Context window size}

Context window size of a LLM refers to the maximum amount of input tokens that the model can process at once. In other words, context window size determines the amount of textual information that a model can consider when making predictions. Larger context windows allow models to maintain coherence over longer text spans, improve understanding of complex dependencies, and generate more contextually appropriate responses. Conversely, smaller context windows might limit the model's ability to capture long-range dependencies, leading to less coherent outputs.


\subsection{Temperature}

The temperature is a hyperparameter that controls the randomness of the model's output. A high temperature results in a more diverse and creative output, whereas a low temperature produces more deterministic and focused responses.


\subsection{Using LLMs}

Despite advanced capabilities of LLMs, the generic nature of LLMs presents challenges when it comes to specific tasks. To improve LLM's output quality for these tasks there are mainly three options. (1) Training own LLMs, (2) fine-tuning an existing LLM and (3) using prompt engineering.

Training own LLMs requires a tons of resources thus it is more viable to fine-tune an existing LLM. Fine-tuning of a LLM means updating it's weights by training on many labelled examples specific to the desired task \cite{Brown2020}. The main advantage of this approach is a strong performance on many benchmarks \cite{Brown2020}. However, the disadvantages are that fine-tuning requires a lot of labelled training data and can still be computationally expensive \cite{Brown2020}. Therefore, the prompt engineering is usually used by providing a prompt to the LLM.


\section{Prompts}

A prompt in the context of LLMs refers to the input provided to the LLM. The prompt sets the context for the LLM, guiding it to produce a relevant and coherent output based on the given information.

A prompting technique is a strategy used to structure or format the prompt to increase the quality of the output by aligning the prompt with the LLM's training and capabilities.


\subsection{General tips}
\label{prompt_general_tips}

There exists a lot of useful general tips for writing prompts independent on the chosen prompting technique. In our experience these tips helped mostly when working with less quality LLMs as the more quality LLMs are less sensitive to a slight differences in the prompt. The following tips are compilation of what we found useful from the ``Microsoft's prompt engineering guide\footnote{\url{https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions}}'' and ``OpenAI's prompt engineering guide\footnote{\url{https://platform.openai.com/docs/guides/prompt-engineering}}''. \\

TODO: Toto by se možná hodilo až do vysvětlení našeho přístupu pro zdůvodnění některých prompt design rozhodnutí. \\


\subsubsection{Starting or ending with the most important instructions}
LLMs place the greatest emphasis on the information at the start and at the end of the prompt. Therefore, defining the main task at the start of the prompt and then repeating it at the end can increase the output quality.


\subsubsection{Adding clear consistent syntax}
Adding clear and consistent syntax makes the prompt more human readable and also can improve the output quality.


\subsubsection{Breaking the task down}
LLMs usually perform better when they are given order in which to generate the output. Also the order of the steps matter as the LLM generates the next token both based on the prompt and based on it's previous output.


%\subsubsection{Priming the output}
%In our experience the LLMs are usually trained in a way to start their response by some preliminary description of the output. This can be reduced by for example putting ``Output:'' at the end of the prompt.

TODO: Možná něco o tom, že LLM typicky kladou velký důraz na system message, proto je dobré sem také uvést hlavní instrukci \\

TODO: Projít si ty odkazy ve footnotech, jestli tam ještě nenajdu něco důležitého \\


\subsection{Prompting techniques}

The output quality of LLMs largely depends on how they are prompted. The most frequently used prompting techniques are the following.


\subsubsection{N-shot prompting}

$N$-shot or few-shot prompting is a prompting technique where $N$ labelled examples are additionally inserted into the prompt to let the LLM learn from the specific examples but no weights are updated \cite{Brown2020}. For instance, one labelled example could contain an English sentence and the corresponding translated Czech sentence.

One of the challenges is to choose suitable value for the $N$. In some cases, not providing enough labelled examples can lead to a worse output quality as the LLM does not have enough context to learn. On the other hand, providing too many labelled examples can over-train the LLM and lead to generating specifically tailored responses to those labelled examples rather than generalizing to a new scenario. Also, when working with a small context window size too many labelled examples can take all the available space.

For example, \citet{Brown2020} compares performance of fine-tuned LLMs and GPT-3 with zero-shot, one-shot and N-shot prompting on many different tasks. In some cases, the N-shot settings almost matches the performance of state-of-the-art fine-tuned LLMs.


\subsubsection{Chain of thoughts}

Chain of thoughts (CoT) prompting technique instructs the LLM to generate a sequence of coherent steps that lead to the final answer of a given task \cite{Wei2022}.

For example, \citet{Wei2022} shows that on many tasks CoT prompting technique improves performance of a LLM compared with N-shot prompting. \\

TODO: Rozšířit info podle paperu "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models". Například uvést, že aby LLM generoval step-by-step řešení, tak lze udělat více způsoby, jako například přidat do promptu "Think step by step", ale to i zkombinovat s N-shot promptingem a přímo vložit konkrétní examply se step-by-step řešením. \\

TODO: Možná vyjmenovat nějaké další možné přístupy, jako například automatic CoT, self-consistency CoT atd. \\


\subsubsection{Tree of thoughts}

The extension of the CoT technique is the tree of thoughts (ToT) prompting technique \cite{Long2023}. \\

TODO: trochu více popsat ToT, podobně jako u CoT rozebrat jednotlivé přístupy, jak lze dosáhnout toho, aby LLM generovat více větví myšlenek atd., hlavní zdroj: paper "LLM guided ToT" \\


\section{Retrieval-augmented generation}

To further improve performance of the LLMs when using prompting techniques, methods such as retrieval-augmented generation can be used.

\subsection{Motivation}

When a LLM generates output, it relies on its trained parameters, which may contain outdated information. This occurs because, after the training phase, these parameters are no longer updated. Consequently, the output produced by the LLM can be outdated or irrelevant. To address this issue, retrieval-augmented generation (RAG) can be employed to incorporate external knowledge into the LLM's output generation process.


\subsection{RAG work-flow}

The RAG process typically involves three main steps \cite{Gao2023}: First, the creation of an external knowledge base. Second, the identification of relevant information within this knowledge base based on the user's input. And third, the generation of a prompt that combines the user's input with the retrieved relevant information from the external knowledge base. \\

NOTE: Ty jednotlivé body můžu klidně více rozepsat podle kroků: "Indexing", "Retrieval", "Generation" v \cite{Gao2023} \\

NOTE: Tady můžu nakreslit nějaký obrázek, obsahově by se sem hodilo něco jako tento\footnote{\url{https://pvml.com/wp-content/uploads/2024/04/rag.png}}


\subsection{Models}

Typically, in the first RAG step the chunks are embedded into a vector database by usually some small-scale language model typically based on Bidirectional Encoder Representations from Transformers (BERT). And in the next step the user's instruction is embedded into the vector space with the same language model. Then the language model computes the distance between the embedded instruction and each of the chunks and the closest $k$ chunks are retrieved.

There are many language models that can be used for text embedding into a vector space. They mainly differ in their input format such as length of the input and whether the given query should be in form of a question or not. Also other methods can be used such as using the LLM to output the $k$ closest chunks.
