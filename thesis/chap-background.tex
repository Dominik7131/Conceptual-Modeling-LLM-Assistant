\chapter{Background}

TOOD: Možná tuto celou kapitolu pak bude lepší nějak zakomponovat do intra

\section{Large language models}

Něco jako: "Language modeling is a traditional task in natural language processing that aims to estimate the conditional probability of a sequence of tokens. Recently, large language models (LLMs) have gained significant attention for this task. LLMs use deep neural networks, typically with transformer architecture [16], to estimate this probability distribution. Given a sequence of tokens $s = \{ s_1, s_2, \ldots, s_{k-1} \}$, LLMs estimate the conditional probability of the next token $P(sk \mid s1,\ldots, s_{k-1})$.
These models are typically used for text generation through auto-regression. Specifically, in each time step, the LLM predicts a new token that is added to the input. As the scale of LLMs (i.e., the number of parameters) increases, some models can be fine-tuned to perform other specific tasks beyond text generation [17]." (zdroj: Automated Domain Modeling with LLMs A Comparative Study) \\

TODO: Asi zde popsat, co je to context window size, protože pak o tom mluvím v kapitole o promptech a o RAGu

TODO: Asi sem nejspíš přijde vyjmenování těch LLM, které používáme plus na ně dám odkazy \\

TODO: Pokud uvedu jméno Mixtralu tak bych mohl přidat kratší sekci o tom, jak funguje mix of experts LLM \\


\section{Prompts}
Něco jako: "In the field of LLMs, a prompt is an input that guides the model’s response
 generation. A prompting technique entails the strategic formulation of these
 prompts to maximize the efficacy of LLMs. It involves the deliberate structuring
 and phrasing of prompts to align with the model’s training and capabilities." (zdroj: Navigating Ontology Development with LLMs) \\