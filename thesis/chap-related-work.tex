\chapter{Related work}

Automated domain modeling encompasses two primary research directions: rule-based methods and statistical methods.

Rule-based methods usually contain hand-written rules and heuristics to automatically identify model elements in a given domain description. For example, \citet{Raharjana2021} and \citet{Sonbol2022} provide recent field surveys for further details.

Statistical methods typically use machine learning techniques to based on a given domain description to either generate the complete domain model \cite{Chen2023,Saeedizade2024} or to start from some smaller conceptual model and based on the user's instructions to iteratively generate a completed conceptual model \cite{Camara2023} or to suggest only specific components that the user can build his conceptual model with.

In the section \ref{section:ref_recent_approaches_using_llms} we dive deeper into recent approaches to automated domain modeling with LLMs and then in the section \ref{section:llm_as_an_assistant} we examine approaches that use LLMs as an assistant. \\

NOTE: Zde můžu něco více rozvést především podle sekce "Related work" v našem článku


\section{Recent approaches using LLMs}
\label{section:ref_recent_approaches_using_llms}

\citet{Camara2023} experiment with ChatGPT-3.5 to generate domain models in PlantUML format. Their experiments demonstrate that generating complete conceptual models comprising more than 8 to 10 classes results into unusable conceptual models. However, they show that it works better to start from a smaller conceptual model and then iteratively enhance it based on user's instructions.

\citet{Fill2023} and \citet{Haerer2023} additionally explore abilities of ChatGPT-4 in generating conceptual models and they experiment with different output formats. \citet{Fill2023} conclude that the LLMs show enormous potential for modeling tasks and the \citet{Haerer2023} concludes that their results show that iterative modeling in a conversational dialogue could be practical, however, further systematic evaluations need to be conducted.

\citet{BabaeiGiglou2023} experiment with different types of LLMs and concludes that ChatGPT-4 usually provides the best output quality for conceptual modeling tasks.

\citet{Chen2023} evaluate generating classes, attributes and associations with 0-shot prompting, N-shot prompting and chain of thoughts with ChatGPT-3.5 and ChatGPT-4. They conclude that the combination of N-shot prompting with ChatGPT-4 achieved the best results however, they also add that fully domain modeling still remains impractical.

\citet{Saeedizade2024} try many different step by step prompting methods to iteratively generate complete conceptual model. Their experiments reveal that using their prompting technique `` Competency Question by Competency Question'' with GPT-4 outperforms the average quality of the initial submission of novice ontology engineering students. \\


NOTE: Některé věci pak tady můžu více rozepsat na základě toho, co pak budu říkat v kapitole o promptech, abych odůvodnil, proč například používámě nějakou promptovací techniku \\


\section{Using LLM as an assistant}
\label{section:llm_as_an_assistant}

Applications that use LLM as an assistant such as GitHub Copilot, Microsoft Office Copilot, Grammarly, Notion AI, etc. has the following things in common.

First, the LLM provides only suggestions that the user can accept, reject, or in some cases regenerate a new suggestion. The reason is that the LLMs still make a lot of mistakes and directly applying their output can lead to a lot of mistakes that can overtime accumulate into even bigger mistakes.

Second, the suggestions are usually in a form where the user can quickly decide if he wants to use it. For example, in Grammarly each mistake is underlined and a reason is provided. In GitHub Copilot it is usually an easy task to decide if suggested comment or code or etc. is relevant.


\section{Our approach}

TODO: Na základě zmíněných related worků zdůvodním naše designová rozhodnutí, tedy:
\begin{itemize}
\item nechceme generovat kompletní konceptuální modely, protože to LLM ještě neumí
\item mohl bych tu definovat problém generování tříd, atributů, asociací jako to je v našem článku
\item protože se LLM kdykoliv můžou splést, tak nechceme, aby LLM iterativně přímo vylepšoval nějaký existující konceptuální model, ale místo toho budeme pouze generovat návrhy, které jsou na uživateli, jestli je využije a nebo ne.
\item tady bych asi rovnou mohl zmínit, že z těch studií vyplývá, že pro začátek je lepší použít nějaký předtrénovaný LLM, než si trénovat vlastní buď úplně from scratch a nebo fine-tunovat, hlavně protože na to ani nemáme data \\
\end{itemize}

TODO: Na základě zmíněných použití asistenta zdůvodním, že:
\begin{itemize}
\item chceme uživatelům zjednodušit rozhodování u každého návrhu, jestli ho použijí, nebo ne, proto budeme chtít generovat i "original text" k návrhům ve formě přesných citací z popisu domény, abychom je pak rovnou mohli uživatelům ukázat \\
\end{itemize}


\section{Approaches to using own LLM}

\subsection{Training own LLM}
- cons: requires a ton of resources: money, time, data


\subsection{Fine-tuning existing LLM}
- cons: requires a lot of data


\subsection{Prompt engineering of existing LLM}
- pros: doesn't require that much resources \\


--- NOTE: Ideálně by výsledkem této kapitoly bylo odůvodnění našeho přístupu, abychom se pak už následně jenom mohli věnovat tomu, co jsme udělali ---