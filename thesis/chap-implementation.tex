\chapter{Generators configuration}

%Let us now use the delineated framework to implement a concrete modeling assistant. We define the generators with prompt templates based on the meta-template. This includes defining (1) main control instructions, modeling procedure specifications, and output specifications, (2) zero or more examples for N-shot prompting, and (3) a technique for context specification. All our prompt templates can be found in our GitHub repository\footnote{\url{https://github.com/Dominik7131/Conceptual-Modeling-LLM-Assistant/tree/master/prompts}}.

\noindent{}TODO: Brief chapter description


\section{Structured data format}

In the prompt template for structured data we use the JSON format as it is one of the most popular formats for data interchange on the internet that each LLM should be greatly familiar with since most of the training data for LLMs usually come from the internet \cite{Zhao2023}.


\section{Output specification}

For automatic result parsing, the \emph{output specification} part instructs the LLM to output the result in a JSON format. To get JSON output from the LLM either a concrete JSON instance can be used or a JSON schema can be used to formally define the JSON format. For example, when the goal is to get output from the LLM of a class \textit{description} in JSON format, the \emph{output specification} instruction containing an instance of JSON format can look like this: \\

\noindent{}\textit{Output the description in JSON format like this:} \\
\texttt{\frenchspacing\{ \\
\null \quad ``description'': ``description of the class'' \\
\}} \\

\noindent{}or the \emph{output specification} instruction containing an JSON schema can look like this: \\

\noindent{}\textit{Output the description in JSON format based on this JSON schema:} \\
\texttt{\frenchspacing\{ \\
\null \quad ``\$schema'': ``http://json-schema.org/draft-04/schema\#'', \\
\null \quad  ``type'':``object'', \\
\null \quad  ``properties'': \{ \\
\null \quad \quad ``description'': \{ \\
\null \quad \quad \quad ``type'': ``string'' \\
\null \quad \quad   \} \\
\null \quad  \}, \\
\null \quad  ``required'': [ \\
\null \quad \quad   ``description'' \\
\null \quad  ] \\
\}} \\


The application response time can be improved by instructing the LLM to output one isolated JSON object for each outputted domain element. This way as soon as the LLM generates some proper domain element it can be displayed to the user. To achieve this, we use in the \emph{output specification} instruction an instance of the corresponding JSON output. For example, when generating classes, the \emph{output specification} instruction can look like this: \\

\noindent{}\textit{Output each class in JSON object like this: \texttt{\frenchspacing\{``class'': ``class name''\}}} \\

\noindent{}As a result, when the suggested classes are \textit{employee} and \textit{department} instead of getting the output in the following single object: \\

\noindent{}\texttt{\frenchspacing\{``classes'': [\{``name'': ``employee''\}, \{``name'': ``department''\}]}\} \\

\noindent{each class is outputted in a separate object:} \\

\noindent{}\texttt{\frenchspacing\{``class'': ``employee''\} \\
\{``class'': ``department''\}}


\section{Output order}
\label{sec:output_order}

When the task of modeling domain elements solely based on a given domain description is done by the modeling experts, they typically proceed in the following two steps: (1) they find the context for the given domain element and (2) from the found context they extract the specific information such as the name of the domain element. For example, consider the following simple domain description: \\

\noindent{}\textit{``In this company, every employee works in some department. Each employee is uniquely identified by his ID.''} \\

\noindent{}When the task is to find associations for the class \textit{department} usually first step is to identify the context for this class. This context can be for example the first sentence of the domain description: \textit{``In this company, every employee works in some department''}. Subsequently, this context is identified and the association ``\textit{works in}'' between the classes \textit{employee} and \textit{department} can be found.

To mimic this approach, when generating classes, attributes, and associations with the LLM in the \emph{output specification} instruction we first specify to generate the original text for the domain element and then its other fields such as the name. For example, the \emph{output specification} instruction for generating attributes can look like this: \\

\noindent{}\textit{Output each attribute in JSON object like this:} \\
\texttt{\frenchspacing\{``originalText'': ``copy the part of the given context containing \\ this attribute'', ``name'': ``attribute name''\}.}


\section{Main instruction}

The table \ref{tab:main-control-instructions} shows examples of our main control instructions. These instructions are independent of the prompting techniques. \\

\noindent{}TODO: Zkusit okomentovat ty main control instructions z tabulky

\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{}l>{\raggedright\arraybackslash}p{0.9\textwidth}>{\raggedright\arraybackslash}p{0.5\textwidth}@{}}
         & main control instruction \\
    \toprule
    \addlinespace
    
$gen_c$ & extract all class names for a UML diagram. \\
\addlinespace

$gen_a$ & generate all attributes for the class: ``\{source\_class\}''. \\
\addlinespace

$gen_{r1}$ & which associations does the class: ``\{source\_class\}'' have? \\
\addlinespace

$gen_{r2}$ & which associations are explicitly between the source class ``\{source\_class\}'' and the target class ``\{target\_class\}''? \\
\addlinespace

$gen_{ad}$ & provide description for the attribute: ``\{attribute\_name\}'' of the class: ``\{source\_class\}'' and output it in this JSON object: \{``description'': ``''\}. \\
\addlinespace

	\bottomrule
	\addlinespace
	\end{tabular}
	\caption{Examples of main control instructions}
	\label{tab:main-control-instructions}
\end{table}


\section{Modeling procedure}

For generating classes, attributes, and associations we implemented the following approaches: baseline, CoT, N-shot, CoT + N-shot. Additionally, for generating classes we implemented the ToT approach.



\subsection{Chain of thoughts}

We experiment with the CoT prompting technique for generating classes, attributes, and associations. First, we need to come up with reasoning steps that the LLM can generate before it outputs each domain element. Some possible reasoning steps can be generated by the LLM by adding the ``\textit{Let's think step by step}'' phrase into the prompts for generating classes, attributes, and associations as discussed in the section \ref{sec:chain_of_thoughts}. Figure \ref{fig:cot-think-step-by-step} shows one of the possible reasoning steps when using ChatGPT-4o with the simple domain description from the section \ref{sec:simple_domain_description_example} for generating attributes.

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.6]{img/cot-think-step-by-step.png}
    \caption{\centering Example of automatically generated reasoning steps by ChatGPT-4o for finding attributes in a simple domain description}
    \label{fig:cot-think-step-by-step}
\end{figure}

As we can see, ChatGPT-4o for each attribute first rephrased some parts of the domain description, and then based on these parts it identified the possible attributes. Note that this approach is coherent with the output order that we specified in the section \ref{sec:output_order}. Inspired by this result, we implemented a simple CoT strategy that instructs the LLM to for each domain element first generate its original text, then generate all other fields such as the name and at the end generate all these previous items in a single JSON object. The table \ref{tab:cot-prompt-templates} shows the modeling procedures and the output specifications for $gen_c$, $gen_a$, $gen_{r1}$, and $gen_{r2}$.

% Other more sophisticated CoT strategies can be used. For example, when the LLM generates attributes to instruct the LLM to add for each generated domain element a reason why it thinks it is an attribute.

\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{}l>{\raggedright\arraybackslash}p{0.4\textwidth}>{\raggedright\arraybackslash}p{0.5\textwidth}@{}}
         & modeling procedure & output specification \\
    \toprule
    \addlinespace
$gen_c$ & For each class copy the part of the given context containing this class and output its name and then output this class in JSON object. & The output should look like this: \newline
context: copy the part of the given context containing this class \newline
name: class name \newline
JSON object: \{``originalText'': ``copy the part of the given context containing this attribute'', ``name'': ``class name''\}. \\
\addlinespace

$gen_a$ & For each attribute copy the part of the given context containing this attribute and output its name and then output this attribute in JSON object. & The output should look like this: \newline
context: copy the part of the given context containing this attribute \newline
name: attribute name \newline
JSON object: \{``originalText'': ``copy the part of the given context containing this attribute'', ``name'': ``attribute name''\}. \\
\addlinespace

$gen_{r1}$ & For each association output its name and source class and target class and copy the part of the given context containing this association in JSON object. &
The output should look like this: \newline
context: copy the part of the given context containing this association \newline
name: association name \newline
source class: source class name \newline
target class: target class name \newline
\{``originalText'': ``copy the part of the given context containing this association'', ``name'': ``association name'', ``source'': ``source class name'', ``target'': ``target class name''\} \\
\addlinespace

$gen_{r2}$ & For each association output its name and source class and target class and copy the part of the given context containing this association in JSON object. &
The output should look like this: \newline
context: copy the part of the given context containing this association \newline
name: association name \newline
source class: \{source\_class\} \newline
target class: \{target\_class\} \newline
\{``originalText'': ``copy the part of the given context containing this association'', ``name'': ``association name'', ``source'': ``\{source\_class\}'', ``target'': ``\{target\_class\}''\} \\

	\addlinespace
	\bottomrule
	\addlinespace
	\end{tabular}
	\caption{Example of CoT modeling procedures and output specifications}
	\label{tab:cot-prompt-templates}
\end{table}


\subsection{N-shot prompting}

We use examples based on the domain description and its domain model that is shown in the figure \ref{fig:prompting-domain}.

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.6]{img/prompting-domain.pdf}
    \caption{\centering The simple company employees domain description and its corresponding domain model used for N-shot prompting in our generator templates}
    \label{fig:prompting-domain}
\end{figure}


For $gen_c$, we use the three classes as examples. For $gen_a$, we use the colored attributes, each with the colored part of the text as examples of the corresponding original texts. For $gen_{r1}$ and ${gen_{r2}}$, we proceed similarly, but for ${gen_{r1}}$ we provide each sample association twice – for the source class and for the target class.

The concrete examples can be used to further specify the output format. For example, when the LLM is not provided with a specific name format for the corresponding domain elements, the outputted names can sometimes be in a snake case convention and some other time in a camel case convention. But when the provided examples contain a consistent naming format, the LLM usually outputs the provided format consistently. Similarly, N-shot prompting technique lets us specify the naming style. For example, when no naming style is provided for attributes, they can contain some unwanted words such as starting with the word ``has'' which is more common for association names.


\subsection{CoT + N-shot prompting}

Our combination of the CoT and the N-shot prompting technique contains the same modeling procedure as shown in the table for only CoT approach. However, the output specification part contains only the expected JSON object, and the intermediate steps are demonstrated in the part where the $N$ examples are provided. For example, the labeled example for the attribute \textit{role} of the class \textit{employee} looks like this: \\

\noindent{}\textit{context: has a specific role} \\
\textit{name: role} \\
\textit{JSON object: \{``originalText'': ``has a specific role'', ``name'': ``role''\}} \\

\noindent{}And the labeled example for the association \textit{manages} with the source class \textit{manager} and the target class \textit{department} looks like this: \\

\noindent{}\textit{context: make sure the departments they manage are productive} \\
\textit{name: manages} \\
\textit{source class: manager} \\
\textit{target class: department} \\
\textit{JSON object: \{``originalText'': ``make sure the departments they manage are productive'', ``name'': ``manages'', ``source'': ``manager'', ``target'': ``department''\}}


\subsection{Tree of thoughts}

Even though our defined generators do not solve the typical complex tasks that the ToT is usually used for, we use this prompting technique for generating classes in the single prompt fashion as discussed in the section \ref{sec:tree_of_thoughts}.

We do not experiment with any iterative prompting technique as this would significantly increase the response time of our application as discussed in the section \ref{sec:iterative_prompting}.

% Possible iterative technique for generating classes: we could generate multiple lists of class suggestions with multiple prompts. Then we could apply some operation to these lists such as intersection or unification. Or for example, we could remove only those classes that appeared in only one list.


\section{Retrieval-augmented generation}

We consider the domain description $T$ as the external knowledge base and our goal is to insert only the relevant parts of the $T$ in the context specification part of the prompt. We use this technique for the generators of attributes, associations, descriptions, data types, and cardinalities as in these cases only the information about the source class $C$ is needed to generate the correct output. For example, consider the following domain description: \\

\noindent{}``\textit{In this company, every employee works in some department. Each employee is uniquely identified by his ID.}''\\

\noindent{}When we want to extract attributes or associations of the class \textit{department} we are only interested in the first sentence since this is the only sentence that contains information about this class. Therefore in this case we can provide only the first sentence of the domain description into the context specification part of the prompt.

The main goal of the mentioned approach is to reduce the hallucination of the LLM such as reducing irrelevant domain element suggestions by forcing the LLM to focus on the important parts of the domain description. Furthermore, this approach usually reduces the prompt length which can help when working with a domain description that does not fit in the context window size of the LLM.

Now we describe the most significant challenges we encountered when implementing the mentioned RAG approach and the corresponding solutions we implemented to address them.


\subsection{Domain description simplification}

We considered using an LLM to simplify the domain description so the LLM assistant would work only with this simplified version. However, there are a few disadvantages to this approach.

First, the LLM sometimes changes some names of the original domain elements. For example, when we instructed the ChatGPT-4o with a domain description about \textit{aircraft manufacturing} and with the following prompt: \\

\noindent{}``\textit{Simplify each sentence structure in the following text. Make sure to not change any nouns and verbs as we want to keep the names of all domain elements unchanged. This is the following text: \ldots}'' \\

\noindent{}The sentence: ``\textit{Customers represent the clients who purchase the finished aircraft.}'' was simplified into: ``\textit{Customers are clients who buy finished aircraft.}'' This means that the original association ``\textit{client purchases aircraft}'' was changed into ``\textit{client buys aircraft}'' which is semantically the same but it can be a problem if the user wants the suggestions to have the same name as described in the text.

The second disadvantage is that the domain description simplification removes the ability to highlight original text for each suggested domain element in the original description as the LLM works only with the simplified version of the domain description. Therefore, we do not simplify the domain description.


\subsection{Domain description segmentation}

In the RAG indexing phase, the domain description needs to be split into chunks so that each chunk can be evaluated if it is relevant for the given source class. Determining the chunk size is a challenging task since with too big chunks we are risking having irrelevant parts of domain description in the prompt and thus decreasing the LLM performance. On the other hand, with too small chunks we are risking that the chunks will be misclassified as they will not contain enough information about their context to decide if they are relevant.

In the result, we consider each sentence of the domain description as one chunk since one sentence usually contains information about one concept or a few related concepts. The advantage of this approach is that after the chunks evaluation is done it is easy to concatenate the relevant chunks together simply by putting them next to each other in the original order from the domain description. On the other hand, the disadvantage is that for example, if some sentence refers to another sentence with a pronoun then without any additional domain description pre-processing this context is lost after the domain description segmentation.


\subsection{Lack of context}

As mentioned, chunks in the form of isolated sentences can be miss-classified if not enough context is provided. This mostly happens when some sentences contains pronouns that refer to some other sentences. For example, consider the class named \textit{book} and the following domain description: \\

\noindent{}``\textit{The book contains a lot of pages. It is very heavy.}'' \\

\noindent{}Now when classifying the chunk ``\textit{It is very heavy.}'' it most likely will be classified as an irrelevant chunk as in isolation it does not contain any information about the class \textit{book} even though the attribute \textit{weight} of the \textit{book} could be inferred from it.

To solve similar issues, we implemented a simple naive algorithm where each chunk has its metadata, and if a chunk starts with a pronoun we insert in its metadata the previous chunk. Now when a chunk is being evaluated, its metadata are also considered. This means that when classifying the second chunk from the example also the first chunk is present therefore the context is not lost. The disadvantages of this approach are that pronouns that are not at the start of a chunk are not considered. Also, adding a whole previous chunk to a current chunk can in some cases lead to miss-classification if the current chunk is not referencing the whole context of the previous chunk.

A possibly better solution is to use some language model that can accurately solve the co-reference resolution task where each pronoun is replaced with the corresponding words that it references. This way when the second chunk from the example is being classified, the classification algorithm works with this text: ``\textit{The Book is very heavy}'' so all relevant context is provided. \\

Another issue with a lack of context can arise when a text contains some bullet points, such as: \\

\noindent{}``\textit{The book contains:}
\begin{itemize}
\item \textit{info about it's author}
\item \textit{date of publication}'' \\
\end{itemize}

\noindent{}To solve this issue, for each bullet point we put in it's metadata the chunk before the first bullet point which in this case is the chunk: ``\textit{The book contains:}''.

%If the requirement is to correctly classify each chunk then either the RAG can be temporarily disabled or the domain description can be manually edited to remove all problematic constructs.


\subsection{Texts comparison}
\label{texts_comparison}
  
We considered using LLM to directly output the relevant texts from the domain description based on the given source class. However, this process can take a long time as in the worst-case scenario the LLM has to copy the whole domain description to the output.

Instead, we implemented a semantic and a syntactic approach that for a given source class and some part of a domain description compares their similarity.
 
The semantic approach uses an embedding model that converts input texts into a vector space and then compares their similarity.

The syntactic approach uses a language model that converts the given source class and each chunk into lemmas. Subsequently, if the chunk contains the lemmas of the given source class then it is classified as relevant. For example, consider the source class \textit{department} and the following domain description: \\

``\textit{In this company, every employee works in some department. Each employee is uniquely identified by his ID.}'' \\

First, each word in the given source class \textit{department} is converted into a lemma which in this case is the same word \textit{department}. Then, each word in the domain description is converted into the lemmas: \\

``\textit{In this company, every employee \textbf{work} in some department. Each employee \textbf{be} uniquely \textbf{identify} by his ID.}'' \\

The bold words emphasize the differences between the original domain description and the domain description after each word is converted into a lemma. Subsequently, in this case, a chunk is relevant if it contains the word \textit{department}. Which in this case is only the first sentence. We do not enforce the chunk to contain the lemmas in the same order as the given source class. For example, consider the source class \textit{registration application} and the following chunk: \\

\noindent{}``\textit{Application of registration needs to be provided.}'' \\

\noindent{}If the strict lemmas order was enforced then this chunk would be classified as irrelevant for the source class \textit{registration application} which is unwanted behavior.


\subsection{Top k search}
\label{sec:top_k_search}

In traditional RAG systems in the retrieval phase, a fixed number $k$ of the most similar results are retrieved after computing the similarity with the embedding model. However, in our specific application, $k$ is not a fixed number because the domain description may contain a variable number of relevant chunks. To address this variability, for our semantic RAG approach we need to set the similarity score threshold. The challenging part is that the similarity score is always relative to the given input. For example, in one scenario, chunks with a similarity score higher than a certain threshold $x$ may be considered similar, while in another scenario, chunks with a similarity score higher than $x$ may not be considered similar. To mitigate this issue, we set the threshold based on the similarity score of the most similar chunk.
