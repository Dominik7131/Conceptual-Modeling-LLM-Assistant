\chapter{Conclusion}

In this thesis, we presented an approach to designing conceptual modeling assistants based on Large Language Models (LLMs) that help human modeling experts mainly discover classes, their attributes, and binary associations in the given textual domain description.
Compared with recent pioneering work that utilizes LLMs for domain modeling automation, we proposed a generic framework that mainly contains class, attribute, and association generators that focuses on assisting human modelers during the modeling process instead of suggesting the whole domain model.
We defined the generators as prompt templates that can be configured in different and comparable ways.
On the other hand, we did not focus on more complex ontological constructs such as the most recent \cite{Saeedizade2024}.
We presented concrete baseline configurations of the generic generators and demonstrated their practical applicability.
We also introduced a methodology for detailed experimental evaluation of the performance of the generators.
The precision, recall and F1 scores measured on various real-life domain descriptions written in different styles and with different levels of complexity showed that even our baseline configuration is applicable for practical modeling.
We further supported this conclusion through our user-based evaluation using the prototype tool that showed a high level of acceptance among real users with different specializations and levels of seniority.
Our work can be further extended with more advanced generator configurations and future LLMs.
Our experimental evaluation methodology can be used to measure and compare these advanced configurations.