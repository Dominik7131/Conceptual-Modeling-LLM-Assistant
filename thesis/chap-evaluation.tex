\chapter{Evaluation}

\section{Challenges}
\begin{itemize}
\item one element can be modeled in a various ways
\item for our purposes no existing tool for automated evaluating
\end{itemize}


\section{Limitations}
\begin{itemize}
\item we used one domain modeling expert for evaluation
\item we did preliminary experiments on most of the configurations with one shorter and one longer domain description and then used more domain descriptions for the most promising configurations
\end{itemize}


\section{Data}

TODO: Provide footnote link to our GitHub repository with the data \\

In our application we work with domain descriptions to generate suggestions. \\

Data, se kterými pracujeme:
\begin{itemize}
\item popisy domén
\item anotované popisy domén \\
\end{itemize}

TODO:
\begin{itemize}
\item jak data vypadají, tedy jednotlivé styly (educational, atd.)
\item možná jak byla vytvořena (především ručně z nějakého vygenerovaného základu od ChatGPT-3.5)
\item tabulka s počtem tříd, atributů, asociací
\item (jeden popis domény používáme pro N-shot prompting)
\item zdůvodnit, k čemu máme anotované popisy domén
\end{itemize}


\section{LLMs}

\section{Selection}

As running a LLM requires a lot of resources, using LLM through some API typically costs some money. To have more freedom with our experiments we run open-source LLMs locally with LLaMA.cpp server\footnote{\url{https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md}} on a single NVIDIA A100 40GB GPU.

We decided to use a pre-trained open-source LLM because training own model requires a ton of resources and fine-tuning some existing LLM requires a lot of data.

Because of our hardware limitation we use Mixtral-8x7B\footnote{\url{https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/blob/main/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf}} (medium sized LLM) \cite{Jiang2024} and Llama-3-70B\footnote{\url{https://huggingface.co/bartowski/Meta-Llama-3-70B-Instruct-GGUF/blob/main/Meta-Llama-3-70B-Instruct-Q5_K_M.gguf}} (relatively large-size LLM) both in quantized variation where on average they have 5 bits per parameter.

Mixtral-8x7B has a context window size of 32k tokens and it outperforms or matches ChatGPT-3.5 across all evaluated benchmarks. This LLM has in total 47B parameters, which are distributed among 8 experts. To achieve faster output generation, for each generated token 2 of the 8 experts are selected. The selection of the experts can be different for each token \cite{Jiang2024}.

Llama-3-70B has 8k tokens context window size and is allegedly  the first open-source LLM to match the ChatGPT-4's performance. However, as of writing this thesis, Meta's research paper about Llama 3 is not released yet.

For LLM's output comparison, we also frequently used ChatGPT-3.5 through the web UI. \\

NOTE: Potom až otestujeme i ChatGPT-4o, tak sem o tom LLM něco napíšu. \\


\section{User evaluation}

TODO: asi nejspíš obrázek z článku a popis, který v základu bude vycházet z článku

\section{RAG evaluation}

TODO: Co nám vyšlo za recall, precision a F1 skóre, když jsme měřili naše RAG filtrovací varianty \\


\section{Generated domain elements evaluation}
(= generated classes, attributes, associations evaluation)

TODO: subsekce se všemi těmi různými pravidly, podle kterých jsme vyhodnocovali recall a precision \\

\section{Descriptions evaluation}
