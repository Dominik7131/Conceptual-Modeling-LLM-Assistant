\chapter{Experimental evaluation}
\label{chap:evaluation}

In this chapter, we evaluate some of the proposed configurations from chapter \ref{chap:framework_configuration}. First, we describe the data that we used for the evaluation. Then, we evaluate our RAG approaches, the generated suggestions of domain elements, and the generated class descriptions. Finally, we present results of our user-based evaluation of our prototype application.


\section{Evaluation domains}

To assess the quality of our generators, we used six domains with their domain description and manually created reference domain models. These domains can be found in our data repository\footnote{\url{https://github.com/dataspecer/domain-modeling-benchmark}}. Table \ref{tab:reference-model-size} summarizes the size of these reference models.

\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lcccccc}
         & Aircrafts & Papers & Farming & Zoos & Colleges & Vehicles \\
    \toprule
    \addlinespace
         \# classes      & 8  & 7  & 14 & 14 & 63 & 66 \\
         \# attributes   & 10 & 12 & 15 & 26 & 14 & 72 \\
         \# associations & 8  & 9  & 9  & 24 & 67 & 46 \\
    \addlinespace
    \bottomrule
    \addlinespace
    \end{tabular}
    \caption{The size of the reference domain models}
    \label{tab:reference-model-size}
\end{table}


We categorize the domains into two groups.
The first group contains three domains: \emph{aircraft manufacturing}, \emph{conference papers}, and \emph{farming}.
They are characterized by simpler descriptions with a limited scope of concepts.
For each domain, we manually created three semantically equivalent domain descriptions in three different styles.
\emph{Analytical} descriptions are written with analytical precision, describing each concept rigorously, unambiguously, and preferably with a consistent sequence of sentences.
\emph{Eclectic} are written as a collection of less structured knowledge from different stakeholders, each describing the domain from a different point of view.
\emph{Educational} emphasize the most important concepts at the beginning, where only essential aspects are described, followed by adding more details about the concepts later in the description.

The second group contains three other domains: \emph{zoological gardens}, \emph{public colleges}, and \emph{vehicle registration}.
These domains are more \emph{complex}. We experiment with one domain description per domain that we sourced externally and its reference domain model that we created manually based on the description.
The domain of zoological gardens is the simplest of the three.
We retrieved its description from a bachelor student who works on a software application for zoological gardens.
For the domain of public colleges, we combined the English translation of the Czech Higher Education Act\footnote{\url{https://www.msmt.cz/areas-of-work/tertiary-education/national-accreditation-bureau-for-higher-education?highlightWords=higher+education+act}} and the Code of study and examination of our university\footnote{\url{https://cuni.cz/UKEN-726.html}}.
We extracted only parts related to the structure and bodies of public colleges, study programs, curricula, subjects, and examinations.
For the domain of vehicle registrations, we combined the English translation of the Czech Act on the Conditions of Operation of Vehicles on Roads and the corresponding Directive of the European Council\footnote{European Council Directive 1999/37/EC of 29 April 1999 on the registration documents for vehicles, \url{https://eur-lex.europa.eu/eli/dir/1999/37/oj}}.
We extracted only parts related to the registration information for road vehicles.


\subsubsection{Annotated domain descriptions}

To assess the quality of our RAG approaches, for each domain description we also manually created corresponding annotated domain description. Each annotated domain description contains tags that for each domain element from the corresponding reference domain model determine the parts that contain information about this domain element. Here is an example of the original domain description and the corresponding annotated domain description from the \emph{farming} domain: \\

\noindent{}Domain description: \textit{A farmer is an individual engaged in agriculture, growing and harvesting crops, and is identified uniquely by a name that is used to refer to the farmer from various documentation, statistical reporting, etc.} \\

\noindent{}Annotated domain description: \textit{\textbf{<farmer>}A farmer is an individual engaged in agriculture, growing and harvesting crops\textbf{</farmer>}, and \textbf{<name>}is identified uniquely by a name that is used to refer to the farmer from various documentation, statistical reporting, etc.\textbf{</name>}} \\

The corresponding reference model contains the class \textit{farmer} with the attribute \textit{name}. In the annotated text, the parts between the opening tag \textbf{<farmer>} and the closing tag \textbf{</farmer>} capture the information about the class \textit{farmer}. Similarly, the text between the opening tag \textbf{<name>} and the closing tag \textbf{</name>} contains information about the attribute \textit{name} with the source class \textit{farmer}.

The annotated domain description can contain multiple tags with the same name. After an opening tag can follow another opening tag if it has a different name but each tag must be first opened and then closed.


\section{RAG evaluation}
\label{sec:filtering_evaluation}

\subsection{Evaluation methodology}

As mentioned, for the RAG evaluation we use the annotated domain descriptions. In the section \ref{sec:rag_configurations}, we introduced our semantic and syntactic RAG approach. For comparing these approaches, we also introduce the no-filtering approach where the domain description is not filtered at all.


\subsubsection{Recall}

For a domain element $e$ let $X_e$ be the set of expected relevant parts that are marked by the tags with the name equal to the $name(e)$. Let $Y_e \subseteq X_e$ be the set of parts from $X_e$ that the selected RAG approach marked as relevant based on the $name(e)$ if $e$ is a class, otherwise based on the $name(source(e))$. Now when given a domain description with its reference domain model containing a set of domain elements denoted by $E$ where it can be a set of classes, attributes, or associations. The recall $R_E$ is computed as:

\[ R_E = \dfrac{\sum_{e \in E}|Y_e|}{\sum_{e \in E}|X_e|}, \]

\noindent{}where $|Y_e|$ denotes the size of the set $Y_e$ and $|X_e|$ denotes the size of the set $X_e$.


\subsubsection{Precision}

For a class $C$ and a domain description $T$ let $X_{C}$ be the set of expected relevant parts from the annotated domain descriptions of the class $C$ and of the attributes and associations with the name of their source class equal to the $name(C)$. Now let $Y_{C}$ be the set of chunks from the given domain description that the selected RAG approach marked as relevant based on the class $C$. Additionally, let $Z_C = X_C \cap Y_C$, i.e. these are the correctly classified chunks by the RAG approach.
Then the precision $P_{\mathcal{C}}$ for the set of classes $\mathcal{C}$ and the domain description $T$ is computed as:

\[ P_{\mathcal{C}} = \dfrac{\sum_{C \in \mathcal{C}}|Z_C|}{\sum_{C \in \mathcal{C}}|Y_C|}. \]

We do not compute the precision separately for classes, attributes, and associations as the goal of our RAG approach is to find relevant information for a given source class but not to find the information about classes, attributes, and associations separately.


\subsection{Selected models}

For our semantic approach, we use the \textit{SentenceTransformers}\footnote{\url{https://sbert.net/}} library with the model \textit{all-MiniLM-L6-v2}\footnote{\url{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}}. It is a symmetric embedding model intended for encoding sentences and short paragraphs. As a comparison function, we use \textit{cosine-similarity}. Our syntactic approach uses the \textit{MorphoDiTa} tool \cite{Strakova2014} with the \textit{english-morphium-wsj-140407} model for converting English words into lemmas \cite{Straka2014}.


\subsection{Results}

The table \ref{tab:filtering-results} contains the measured results. As expected, the no-filtering approach gets the highest possible recall but also gets the lowest precision.

\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lcccc}

    \toprule
       Configuration & Recall classes & Recall attributes & Recall associations & Precision \\
    \toprule
    
    \addlinespace
         no-filtering      & \textbf{1.00}  & \textbf{1.00}  & \textbf{1.00} & 0.06 \\
    	 syntactic-baseline & 0.87 & 0.79 & 0.84 & \textbf{0.55} \\
         syntactic-pronouns-naive & 0.93  & 0.91  & 0.90 & 0.49 \\
         semantic-baseline & 0.99 & 0.96 & 0.97 & 0.10 \\
         semantic-pronouns-naive & \textbf{1.00} & 0.98 & 0.98 & 0.10 \\
    \addlinespace
    \bottomrule
    \addlinespace
    \end{tabular}
    \caption{RAG approaches evaluation results}
    \label{tab:filtering-results}
\end{table}


The syntactic approach has the highest precision but the lowest recall. The syntactic naive pronouns approach improves the recall significantly but slightly decreases the precision since chunks that start with a pronoun also contain the previous chunk. %This approach can probably offer the best performance as it has the highest precision of all the measured approaches. To improve its recall, the domain description can be manually pre-processed to remove some problematic constructs such as unexpressed subjects.

The semantic approach almost matches the recall of the no-filtering approach but it has a higher precision. The semantic naive pronouns approach slightly increases the recall while maintaining the same precision.

Overall, when recall is the most important then the no-filtering approach or the semantic approach can be used. Therefore, in our application the semantic approach is selected as the default setting. On the other hand, when precision is the most important i.e. we want to reduce the domain description as much as possible and we do not care about losing some relevant parts, then the syntactic approach is the most suitable.


\section{Selected LLMs}

Running a LLM requires a lot of resources, therefore frequently using a LLM through some API can cost a lot of money. Therefore, we run open-source LLMs locally with LLaMA.cpp server\footnote{\url{https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md}} however, our hardware is limited to a single NVIDIA A100 40GB GPU.


\subsection{Limitations}

Because of our hardware limitation, we use Mixtral-8x7B\footnote{\url{https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF/blob/main/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf}} (medium-sized LLM) \cite{Jiang2024} and Llama-3-70B\footnote{\url{https://huggingface.co/bartowski/Meta-Llama-3-70B-Instruct-GGUF/blob/main/Meta-Llama-3-70B-Instruct-Q5_K_M.gguf}} (relatively large-size LLM) both in quantized variation where on average they have 5 bits per parameter. To compare these LLMs with currently one of the best LLM, we use ChatGPT-4o through the OpenAI's web user interface. However, as we cannot connect this LLM into our application for free, we used this LLM only for evaluation of generated classes. Also, for ChatGPT-4o we cannot set the temperature or even get the value of the temperature.


\subsection{Mixtral-8x7B}

Mixtral-8x7B has the context window size of 32k tokens and it outperforms or matches ChatGPT-3.5 across many benchmarks \cite{Jiang2024}. This LLM has in total 47B parameters, which are distributed among 8 experts. To achieve faster output generation, for each generated token 2 of the 8 experts are selected. The selection of the experts can be different for each token \cite{Jiang2024}.


\subsection{Llama-3-70B}

Llama-3-70B has an 8k tokens context window size and currently, it is allegedly the state-of-the-art LLM at the 70B parameters scale according to its authors\footnote{\url{https://ai.meta.com/blog/meta-llama-3/}}.


\subsection{ChatGPT-4o}

ChatGPT-4o is an OpenAI's LLM that has 128k context window size according to the OpenAI's documentation\footnote{\url{https://platform.openai.com/docs/models/gpt-4o}}. OpenAI states that it is their most advanced model however, the number of parameters is not officially published.


\section{Evaluation of generated domain elements}

First, we describe the rules and measurement for evaluating the domain elements then we present a preliminary evaluation where we evaluate generated attributes and associations with many different configurations but with only the domain description \textit{zoological gardens}. Then, we evaluate similar configurations for generated classes but across all our domain descriptions. Finally, we select the most successful configurations and we evaluate them all together on all our domain descriptions.


\subsection{Manual assessment of generated suggestions}

Each generated suggestion was manually assessed by a human evaluator who recorded the assessment in the evaluation tables in columns \emph{Matches class}, \emph{Matches attribute}, and \emph{Matches association}.
\emph{Semantic correspondence} between a suggested $X'$ and a reference $X$ $\in$ $M$ is essential for the evaluation.
%The evaluator decides based on the metadata provided about $X'$ and $X$.
If the question \emph{Can we model $X$ also as $X'$?} can be answered positively, $X'$ \emph{semantically corresponds to} $X$. To limit subjectivity, we carried out the manual evaluation according to the rules summarized in table \ref{tab:manual-assessment-of-suggestions}.

\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lccc}
             & Matches class & Matches attribute & Matches association \\
\toprule
\addlinespace
        $gen_c(T)$   & $name(X)$ $\vert$ * $\vert$ & \emph{N/A}  & \emph{N/A} \\
                     & $name(X_1)$;$\ldots$        & \emph{N/A}  & \emph{N/A} \\
\addlinespace
\midrule
\addlinespace
        $gen_a(T,C)$ & $name(D)$ $\vert$ *         & $name(X)$ $\vert$ * $\vert$ & $name(X)$ $\vert$ * $\vert$ \\
        $gen_{r1}(T,C)$ &                             & :$name(X)$ $\vert$ :* $\vert$ & :$name(X)$ $\vert$ :* $\vert$ \\
                     &                             & +$name(X_1)$;$\ldots$        & +$name(X_1)$;$\ldots$ \\
\addlinespace
\bottomrule
    \end{tabular}
    \caption{Summary of manual evaluation rules}
    \label{tab:manual-assessment-of-suggestions}
\end{table}


\subsubsection{Rules for classes}
If the class $X'$ suggested by $gen_c(T)$ corresponds to a single class $X$, we record $name(X)$ in \emph{Matches class}.
%The correspondence is determined by whether $name(X')$ refers to the same conceptual entity as $name(X)$.
The assessment considers both the occurrences of $name(X')$ within $T$ and how $X$ is structured within $M$.
If $X'$ does not correspond to any class in $M$, but the evaluator considers it valid from the semantic point of view based on $T$, we record an asterisk (*) in \emph{Matches class}.
%This indicates that according to the judgment of the evaluator, $X'$ represents a concept that could be modeled in $M$ based on its relevance and presence in $T$.
This scenario can arise especially in more complex domains where not all concepts in the domain description are modeled, or a concept can be modeled with an attribute instead of an association and a class.
If $X'$ does not correspond to any single class in $M'$, but its $name(X')$ corresponds to a list of classes $X_1$, $\ldots$, $X_n$ $\in$ $M$, their names are concatenated to $+name(X_1)$;$\ldots$;$name(X_n)$ which we record in \emph{Matches classes}. This corresponds to scenarios where domain models are more coarse-grained.

The table \ref{tab:class-rules} shows examples of usage of these class rules with the \textit{registry of road vehicles} domain. For simplicity, let the reference domain model contain only the following classes:

\begin{itemize}
\item \textit{road vehicle}
\item \textit{agricultural tractor}
\item \textit{forestry tractor}
\end{itemize}

\noindent{}In the first row of the table \ref{tab:class-rules}, the generated class \textit{road vehicle} is recorded in the \textit{Matches class} column as it semantically corresponds to the \textit{road vehicle} class from the reference model. In the second column, the asterisk means that the class \textit{owner} based on the given domain description can be modeled as a class however, in the reference domain model it is not present as a class. And in the third row, as the generated class semantically corresponds to the classes \textit{agricultural tractor} and \textit{forestry tractor}, both of these classes are recorded in the \textit{Matches class} column but with a \textit{+} prefix and separated with a semicolon.

\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lccc}
     \toprule
         Generated class & Matches class \\
    \toprule
    
\addlinespace
road vehicle   & road vehicle \\
\addlinespace
owner         & * \\
\addlinespace
agricultural or forestry tractor & +agricultural tractor;forestry tractor \\
    \addlinespace
    \bottomrule
    \addlinespace
    \end{tabular}
    \caption{Class rules examples}
    \label{tab:class-rules}
\end{table}



\subsubsection{Rules for attributes and associations}

For a property $X'$ suggested by $gen_a(T,C)$ and $gen_{r1}(T,C)$, we consider similar rules with some differences.
If $X'$ is a suggested attribute, it can also correspond to an association $X$ and vice versa.
This scenario arises in practice, where properties can be modeled as attributes or associations interchangably based on the desired level of semantic and structural detail of the resulting domain model.
Therefore, $name(X)$ is recorded in \emph{Matches attribute} if $X$ is a reference attribute or \emph{Matches association} if $X$ is a reference association.
Moreover, $X'$ must correspond to $X$ also in its source class and, if $X'$ is an association, in its target classes.
$X'$ can be suggested in the reverse direction of $X$.
The asterisk (*) and the lists are used in the same way as for the classes.
In addition, when $X'$ corresponds to an association $X$ then $name(D)$ of the class at the other end of $X$ than the end with $C$ is recorded in \emph{Matches class}.
This corresponds to a scenario where a domain modeler models $D$ not in the step of identifying classes in $T$, but when modeling $X$ starting in $C$.
%In this step, the designer creates $D$ which we record by $name(D)$ in \emph{Matches class}.
When $X'$ corresponds to an association that is not in $M$, the class at the other end of this association is a class $D$ in $M$, which we record again with $name(D)$ in \emph{Matches class}.
Last but not least, it might happen that $X'$ corresponds to $X$ only when the correspondence requirement in the source/target class is relaxed so that the classes at the ends of $X'$ can be subclasses or superclasses of the corresponding classes at the ends of $X$.
In that case, we write a colon(:) followed by $name(X)$ to \emph{Matches attribute} or \emph{Matches class}.
We write a colon followed by an asterisk (:*) if $X'$ makes sense, but should be modeled for a subclass or superclass of $C$.

The table \ref{tab:attribute-rules} shows examples of attribute rules. In the first row, the attribute \textit{type} is in the reference domain model however, instead of the source class \textit{vehicle} it has in the reference model the source class \textit{road vehicle}. The class \textit{vehicle} is a subclass of the class \textit{road vehicle} therefore, in the \textit{Matches attribute} column is the \textit{:} prefix. In the second row, the generated attribute \textit{minor defect} is modeled in the reference model as the association \textit{has road vehicle defect} and the class \textit{minor defect} therefore, the \textit{Matches class} and \textit{Matches association} columns contain these records.


\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
\begin{tabular}{@{}l>{\raggedright\arraybackslash}p{0.1\textwidth}>{\raggedright\arraybackslash}p{0.3\textwidth}>{\raggedright\arraybackslash}p{0.1\textwidth}>{\raggedright\arraybackslash}p{0.1\textwidth}>{\raggedright\arraybackslash}p{0.1\textwidth}>{\raggedright\arraybackslash}p{0.1\textwidth}@{}}
         Generated attribute & Source class & Generated original text & Matches class & Matches attribute & Matches association \\
    \toprule
    \addlinespace

type & vehicle & A vehicle is a machine, usually with wheels, used for transporting people, animals or goods, on land. & & :type & \\

\addlinespace

minor defect & road vehicle &  a minor defect which does not significantly affect the operational characteristics of the vehicle & minor defect & & has road vehicle defect \\

	\addlinespace
	\bottomrule
	\addlinespace
	\end{tabular}
	\caption{Attribute rules examples}
	\label{tab:attribute-rules}
\end{table}


\subsubsection{Recall measurement}

For the given domain description $T$ and its reference model $M$ $=$ $({\cal C}, {\cal P})$, let ${\cal C}'$ $=$ $gen_c(T)$ be the set of all suggested classes, ${\cal P}'$ $=$ $\bigcup_{C \in {\cal C}} gen_a(C,T)$ $\cup$ $\bigcup_{C \in {\cal C}} gen_{r1}(C,T)$ the set of all suggested properties, and $M'$ $=$ $({\cal C'}, {\cal P}')$ the suggested model.
Let $eval_c(X')$, $eval_a(X')$, and $eval_r(X')$ denote the assessment recorded for $X'$ in columns \emph{Matches class/attribute/association}, respectively.
We distinguish the following matching functions ${\cal C}' \cup {\cal P}'$ $\times$ ${\cal C} \cup {\cal P}$ $\rightarrow$ $\{\mathtt{True}, \mathtt{False}\}$:
\begin{itemize}
    \item $matches_{strict}(X',X)$ iff $X$ is a class, $X'$ is a suggested class, and $eval_c(X')$ $=$ $name(X)$ (similarly for attributes and associations),
    \item $matches_{constr}(X',X)$ iff $eval_{c \vert a \vert r}(X')$ $=$ $name(X)$ for a suggestion $X'$.
    \item $matches_{isa}(X',X)$ iff $matches_{constr}(X',X)$, or $eval_{a \vert r}(X')$ $=$ :$name(X)$ for a property $X$,
    \item $matches_{list}(X',X)$ iff $matches_{isa}(X',X)$, or $eval_{c \vert a \vert r}(X')$ is a list containing $name(X)$ or :$name(X)$.
\end{itemize}

These functions allow us to distinguish different levels of strictness in determining how much the suggestions must conform to the reference model $M$.
The variant \emph{strict} (shortcut $strict$) requires that the suggestion $X'$ exactly corresponds to $X$, including the correspondence of the suggested modeling construct to the construct actually used in $M$.
The variant \emph{modeling construct agnostic} (shortcut $constr$) relaxes the need to use the exact modeling constructs.
It supports situations where an attribute is suggested instead of association or vice versa and where a class is suggested as a part of attribute or association suggestion.
The variant \emph{ISA agnostic} (shortcut $isa$) relaxes the exact match in the inheritance hierarchy where a property is suggested for a subclass or superclass of the class in $M$.
The variant \emph{list agnostic} (shortcut $list$) relaxes the need to identify individual properties instead of an umbrella property.
The last two variants $isa$ and $list$ add relaxations only for properties, not for classes.

With these matching functions, we define different recall and precision variants.
Recall measures to what extent $M'$ covers $M$ considering the different levels of strictness.
For classes, it is expressed as

%\[R^c_\Box(M',M) = \frac{\sum_{X \in {\cal C}}covered_\Box(M',X)}{\vert {\cal C} \vert}\]

\[R^{\cal C}_\Box(M',M) = \frac{\vert \{X \in {\cal C}: (\exists X' \in M')(matches_\Box(X',X))\} \vert}{\vert {\cal C} \vert}\]

\noindent which is the number of reference classes for which there exists a matching suggestion based on manual evaluation compared to the total number of reference classes.
The strictness of the matching is given by the chosen matching function.
Similarly, we define the recall for attributes and associations.


\subsubsection{Precision measurement}

Precision measures the correctness of $M'$.
$M'$ is correct if each $X'$ $\in$ $M'$ is correct.
$X'$ is correct if it matches some $X$ $\in$ $M$ or if it does not match any $X$ $\in$ $M$ but it is still meaningful compared to $T$ which is recorded in the evaluation tables with the * symbol.
For this, we introduce the following auxiliary functions ${\cal C}' \cup {\cal P}'$ $\rightarrow$ $\{\mathtt{True}, \mathtt{False}\}$ to consider also the asterisk symbols in the evaluation:
\begin{itemize}
    \item $matches^*_{strict}(X')$ iff $X'$ is a suggested class, and $eval_c(X')$ $=$ * (similarly for attributes and associations)
    \item $matches^*_{constr}(X')$ iff $eval_{c \vert a \vert r}(X')$ $=$ *
    \item $matches^*_{isa}(X')$ iff $matches^*_{constr}(X')$, or $eval_{c \vert a \vert r}(X')$ $=$ :*
    \item $matches^*_{list}(X')$ iff $matches^*_{isa}(X')$, or $eval_{c \vert a \vert r}(X')$ is a list with * or :*
\end{itemize}

For classes, the precision is expressed as

\[P^c_\Box(M',M) = \frac{\vert \{X' \in {\cal C}': (\exists X \in C)(matches_\Box(X',X)) \lor matches^*_\Box(X') \}\vert}{\vert {\cal C'} \vert}\]

\noindent which is the number of generated class suggestions that match some reference class or any meaningful element that does not exist in the reference model compared to the total number of suggested classes.
For classes, the precision is strict in the need to match the class suggestion to a reference class.
The precision for attributes is defined as

\[P^a_\Box(M',M) = \frac{\vert \{X' \in {\cal A}': (\exists X \in {\cal P})(matches_\Box(X',X)) \lor matches^*_\Box(X') \}\vert}{\vert {\cal A'} \vert}\]

\noindent and similarly for associations. For attributes and associations, the precision requires the match of a property suggestion to a reference property.


\subsubsection{$F_1$ score measurement}

Finally, for the $F_1$ score measurement, we use the classical $F_1$ score definition:

\[ F_{1} = \dfrac{2 \times P \times R}{P + R}, \]

\noindent where $P$ is precision and $R$ is recall.


\subsection{Preliminary evaluation for attributes and associations}
\label{sec:preliminary_attributes_associations}

For the preliminary evaluation, we show results only for the \textit{strict} variation. The table \ref{tab:preliminary-mixtral} shows preliminary results for generated attributes and associations with the Mixtral-8x7B and the table \ref{tab:preliminary-llama} shows preliminary results for generated attributes and associations with the Llama-3-70B. For the associations, we evaluated only the scenarios where either the source class or the target class is provided but not where both the source class and the target class is provided since these tasks are very similar and should provide similar results.

\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lcccccc}
    \toprule
         & & Attributes & & & Associations & \\
     \toprule
        Configuration & Recall & Precision & $F_1$ & Recall & Precision & $F_1$ \\
    \toprule
    
    \addlinespace
         baseline-none        & 0.73 & 0.33 & 0.45 & 0.54 & 0.22 & 0.31 \\
    	 baseline-semantic    & 0.73 & 0.35 & 0.47 & 0.75 & 0.27 & 0.40 \\
         baseline-syntactic   & 0.73 & 0.38 & 0.50 & 0.62 & 0.38 & 0.47 \\
         CoT-none             & 0.62 & 0.34 & 0.44 & 0.75 & 0.50 & 0.60 \\
         CoT-semantic         & 0.58 & 0.58 & \textbf{0.58} & 0.67 & 0.39 & 0.49 \\
         CoT-syntactic        & 0.88 & 0.43 & \textbf{0.58} & 0.71 & 0.53 & 0.61 \\
         N-shot-none          & 0.85 & 0.23 & 0.36 & 0.71 & 0.35 & 0.47 \\
         N-shot-semantic      & 0.85 & 0.32 & 0.46 & 0.83 & 0.47 & 0.60 \\
         N-shot-syntactic     & 0.88 & 0.38 & 0.53 & 0.83 & 0.59 & \textbf{0.69} \\
         CoT+N-shot-none      & 0.92 & 0.28 & 0.43 & 0.46 & 0.40 & 0.43 \\
         CoT+N-shot-semantic  & 0.88 & 0.35 & 0.50 & 0.79 & 0.49 & 0.60 \\
         CoT+N-shot-syntactic & 0.92 & 0.37 & 0.53 & 0.79 & 0.53 & 0.63 \\
    \addlinespace
    \bottomrule
    \addlinespace
    \end{tabular}
    \caption{Preliminary evaluation for generated attributes and associations with Mixtral-8x7B strict variation only}
    \label{tab:preliminary-mixtral}
\end{table}


\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lcccccc}
    \toprule
        & & Attributes & & & Associations & \\
     \toprule
        Configuration & Recall & Precision & $F_1$ & Recall & Precision & $F_1$ \\
    \toprule
    
    \addlinespace
         baseline-none        & 0.96 & 0.80 & \textbf{0.87} & 0.71 & 0.34 & 0.46 \\
    	 baseline-semantic    & 0.88 & 0.64 & 0.74 & 0.79 & 0.49 & 0.60 \\
         baseline-syntactic   & 0.92 & 0.59 & 0.72 & 0.89 & 0.59 & 0.69 \\
         CoT-none             & 0.88 & 0.72 & 0.79 & 0.67 & 0.59 & 0.63 \\
         CoT-semantic         & 0.85 & 0.70 & 0.77 & 0.88 & 0.56 & 0.68 \\
         CoT-syntactic        & 0.92 & 0.52 & 0.66 & 0.88 & 0.54 & 0.67 \\
         N-shot-none          & 0.92 & 0.66 & 0.77 & 0.79 & 0.60 & 0.68 \\
         N-shot-semantic      & 0.88 & 0.57 & 0.69 & 0.88 & 0.56 & 0.68 \\
         N-shot-syntactic     & 0.92 & 0.66 & 0.77 & 0.83 & 0.60 & 0.70 \\
         CoT+N-shot-none      & 0.92 & 0.77 & 0.84 & 0.71 & 0.57 & 0.63 \\
         CoT+N-shot-semantic  & 0.85 & 0.64 & 0.73 & 0.79 & 0.60 & 0.68 \\
         CoT+N-shot-syntactic & 0.92 & 0.66 & 0.77 & 0.83 & 0.67 & \textbf{0.74} \\
    \addlinespace
    \bottomrule
    \addlinespace
    \end{tabular}
    \caption{Preliminary evaluation for generated attributes and associations with Llama-3-70B strict variation only}
    \label{tab:preliminary-llama}
\end{table}

We can see that with Mixtral-8x7B the semantic and syntactic RAG improves precision for both attributes and associations. However, in the case of Llama-3-70B the RAG does not seem to have any noticeable effect probably because Llama-3-70B is able to find attributes and associations without the domain description filtering when working with the \textit{zoological gardens} domain description which has almost 1000 words since the baseline approach without domain description filtering already achieves a high recall and precision. %The baseline prompting technique is the most inconsistent as it has one of the worst $F_1$ scores every time except for the case with \emph{Llama-3-70B} for attributes where it achieves the best $F_1$ score.

At first, our proposed techniques do not show any significant improvement but when we look at their average $F_1$ scores in the table \ref{tab:F1_attributes_associations_average}, we see that the CoT+N-shot configuration gives consistently good results therefore, in the section \ref{sec:overall_evaluation} we focus on this prompting technique and evaluate it with all our domain descriptions.


\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lc}
     \toprule
        Configuration & $F_1$ average \\
    \toprule
    
    \addlinespace
    baseline-none  & 0.52  \\
    baseline-semantic  & 0.55  \\
    baseline-syntactic  & 0.59  \\
    CoT-none	& 0.61  \\
    CoT-semantic & 0.63  \\
    CoT-syntactic & 0.63  \\
    N-shot-none & 0.57 \\
    N-shot-semantic & 0.61 \\
    N-shot-syntactic & \textbf{0.67} \\
    CoT+N-shot-none & 0.58 \\
    CoT+N-shot-semantic & 0.63 \\
    CoT+N-shot-syntactic & \textbf{0.67} \\
    \addlinespace
    \bottomrule
    \addlinespace
    \end{tabular}
    \caption{Average $F_1$ score for generated attributes and associations with Mixtral-8x7B and Llama-3-70B with strict variation only}
    \label{tab:F1_attributes_associations_average}
\end{table}


\subsection{Classes evaluation}

For generated classes the table \ref{tab:mixtral-classes} shows results for Mixtral-8x7B and the table \ref{tab:llama-classes} shows results for Llama-3-70B. Both these tables present the results only for the \emph{strict} variation.

\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lccc}
    \toprule
         & & Classes & \\
     \toprule
        Configuration & Recall & Precision & $F_1$ \\
    \toprule
    
    \addlinespace
         baseline    & 0.49 & 0.99 & 0.66 \\
    	 CoT         & 0.38 & 0.99 & 0.55 \\
         N-shot      & 0.65 & 0.84 & \textbf{0.73} \\
         CoT+N-shot  & 0.45 & 0.96 & 0.61 \\
         ToT         & 0.53 & 0.99 & 0.69 \\
    \addlinespace
    \bottomrule
    \addlinespace
    \end{tabular}
    \caption{Evaluation of generated classes with Mixtral-8x7B strict variation}
    \label{tab:mixtral-classes}
\end{table}


\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lccc}
    \toprule
    & & Classes & \\
    \toprule
        Configuration & Recall & Precision & $F_1$ \\
    \toprule
    
    \addlinespace
         baseline    & 0.63 & 0.98 & \textbf{0.77} \\
    	 CoT         & 0.46 & 0.99 & 0.63 \\
         N-shot      & 0.54 & 0.99 & 0.70 \\
         CoT+N-shot  & 0.48 & 0.94 & 0.64 \\
         ToT         & 0.54 & 0.96 & 0.69 \\
    \addlinespace
    \bottomrule
    \addlinespace
    \end{tabular}
    \caption{Evaluation of generated classes with Llama-3-70B strict variation}
    \label{tab:llama-classes}
\end{table}




As we can see, the CoT and the CoT+N-shot variations have the worst $F_1$ scores. In these prompting techniques, the LLM for each class first generates its original text and then its name while in the other evaluated prompting techniques the LLM for each class generates only its name. Therefore, it seems that generating the original text first lowers the $F_1$ score mainly by decreasing the recall. Therefore, in our application, we do not generate original text for the classes. The table \ref{tab:average-classes} shows average $F_1$ scores for Mixtral-8x7B and Llama-3-70B combined.


\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lc}
    \toprule
        Configuration & $F_1$ average \\
    \toprule
    
    \addlinespace
         baseline   & 0.71 \\
         CoT        & 0.59 \\
         N-shot     & \textbf{0.72} \\
         CoT+N-shot & 0.62 \\
         ToT        & 0.69 \\
    \addlinespace
    \bottomrule
    \addlinespace
    \end{tabular}
    \caption{Average $F_1$ score for Mixtral-8x7B and Llama-3-70B strict variation}
    \label{tab:average-classes}
\end{table}

As we can see, the N-shot prompting technique has the highest $F_1$ score but it is only slightly better then using the baseline configuration. To compare the results with one of the best LLMs, we evaluated the three best variations with the highest $F_1$ score with the ChatGPT-4o. The table \ref{tab:chatgpt-classes} shows the results.


\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lccc}
    \toprule
    & & Classes & \\
    \toprule
       Configuration & Recall & Precision & $F_1$ \\
    \toprule
    
    \addlinespace
         baseline    & 0.65 & 0.94 & \textbf{0.77} \\
         N-shot      & 0.62 & 0.99 & 0.76 \\
         ToT         & 0.54 & 0.96 & 0.69 \\
    \addlinespace
    \bottomrule
    \addlinespace
    \end{tabular}
    \caption{Evaluation of generated classes with ChatGPT-4o strict variation}
    \label{tab:chatgpt-classes}
\end{table}

Surprisingly, the Llama-3-70B with the baseline configuration achieves the same $F_1$ score as the ChatGPT-4o with the baseline configuration. Also, the N-shot prompting technique achieves a similar $F_1$ score as the baseline.


\subsection{Overall evaluation}
\label{sec:overall_evaluation}

Now we evaluate the classes, attributes, and associations together on all our domain descriptions. For the classes, we focus only on the N-shot prompting technique since it had on average the highest $F_1$ score with Mixtral-8x7B and Llama-3-70B. For attributes and associations, we focus only on the CoT+N-shot prompting technique as discussed in the section \ref{sec:preliminary_attributes_associations}. For RAG approaches, we consider only the syntactic RAG approach and the approach without filtering domain description for simplicity.

Figures \ref{fig:evaluation-complex-p-r-f1} and \ref{fig:evaluation-simple-f1} summarize the evaluation results with the average $F_1$ score for the analytical, eclectic, and educational domain descriptions and the average precision, recall, and $F_1$ score for the complex domain descriptions separately for classes, attributes, and associations at the four levels of strictness of the matching.

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.10]{img/evaluation-complex-p-r-f1.png}
    \caption{\centering Average Precision, recall and $F_1$ scores for the complex domain descriptions at the four levels of matching strictness}
    \label{fig:evaluation-complex-p-r-f1}
\end{figure}

\begin{figure}[!h]
    \centering
    \includegraphics[scale=0.10]{img/evaluation-simple-f1.png}
    \caption{\centering Average $F_1$ score for the analytical, eclectic, and educational domain descriptions at the four levels of matching strictness}
    \label{fig:evaluation-simple-f1}
\end{figure}


\subsubsection{Overall performance}

The results indicate that Llama-3-70B generally performs better than Mixtral-8x7B.
This difference is clearly visible for the simpler evaluation domains in all description styles.
In contrast, for complex domains with intricate descriptions, the performance gap narrows.
Notably, Mixtral-8x7B with RAG surpasses Llama-3-70B without RAG in suggesting attributes and associations within complex domains.
Both models show the highest performance in suggesting classes.
They are comparable in suggesting attributes and associations, which is given by the duality in their manual evaluation.
The average precision for classes on complex domains is the same with and without RAG, as RAG is not used for classes.


\subsubsection{Impact of introducing RAG}

Introducing RAG improves the performance of both models.
For Mixtral-8x7B, RAG significantly improves the $F_1$ scores for attributes and associations across all domains, making its performance almost comparable to Llama-3-70B without RAG and even slightly better for complex domains.
This suggests that RAG can compensate for the smaller size of the Mixtral-8x7B model when identifying properties modeled as attributes or associations.
This finding is crucial for implementing the generators, since using a large LLM like Llama-3-70B in the generators is very resource-consuming.
Even a simple RAG technique makes less demanding LLM comparable to the larger one for the implementation of the generators.
For Llama-3-70B, the improvement with RAG is less pronounced but still noticeable, particularly for complex domains.
For classes, the impact of RAG is minimal, because the class generator always works with the entire domain description.
We can see some improvements in recall for the less strict levels of matching that also consider classes suggested as part of attribute and association suggestions, where RAG brings improvements.


\subsubsection{Domain and style specific Performance}

Both LLMs perform better in analytic and educational descriptions, probably due to the shorter, more structured, and consistent nature of these descriptions.
The eclectic and complex domain descriptions show more variability in performance.
Both benefit from introducing RAG which shows that diverse and less structured domain descriptions benefit from RAG even for Llama-3-70B.


\section{Class descriptions evaluation}

We evaluated generated class descriptions based on the following questions:

\begin{enumerate}
\item [Q1:] Is the generated description solely based on the given domain description?
\item [Q2:] Is the generated description semantically corresponding to the given domain element?
\item [Q3:] Is the generated description not missing any useful information?
\end{enumerate}

\noindent{}To limit subjectivity of the evaluation, we define the following sets:

\begin{itemize}
\item $T_1 =$ set containing all information about the given class from the given domain description
\item $T_2 =$ set containing the same information as $T_1$ except for detailed information about attributes and associations of the given class
\item $D_1 =$ set containing information from the generated class description based on the given class and the given domain description
\item $D_2 = D_1 \cap T_2 =$ set containing information from the generated class description but without the generated class information that were not solely based on the given class and that contained unrelated information for the given class
\end{itemize}

\noindent{}And we define the following rules:
\begin{itemize}
\item for Q1 if $T_1$ and $D_1$ at least almost fully overlap, we assign 1 point, if they have almost no overlap, we assign 0 points, otherwise we assign 0.5 points
\item for Q2 if the $D_1$ mostly contains information that are related to the given class, we assign 1 point, if the $D_1$ mostly contains unrelated information to the given class, we assign 0 points, otherwise we assign 0.5 points; by related information we mean the information describing the given class but not describing its attributes and associations in detail
\item for Q3, if $T_2$ and $D_2$ at least almost fully overlap, we assign 1 point, if they have almost no overlap, we assign 0 points, otherwise we assign 0.5 points
\end{itemize}


\noindent{}For example, consider the following domain description: \\

\noindent{}\textit{A farmer is an individual engaged in agriculture identified uniquely by a name that is used to refer to the farmer from various documentation.} \\

\noindent{}and consider the following description for the class \textit{farmer}: \\

\noindent{}\textit{Farmer is identified uniquely by a name.}

\begin{itemize}
\item $T_1$ contains the whole domain description
\item $T_2$ contains the following information: farmer is an individual engaged in agriculture, farmer is identified uniquely by a name
\item $D_1$ and $D_2$ contains the whole generated class description
\end{itemize}

\noindent{}As a result we would assign:

\begin{itemize}
\item 1 point for Q1 since $D_1$ and $T_1$ almost fully overlaps in other words, $D_1$ is solely based on the given domain description
\item 1 point for Q2 since $D_1$ contains information only about the given class and it does not describe its attributes and associations or does not contain any other undesired information
\item 0.5 points for Q3 since the overlap of $T_2$ and $D_2$ is about a one half. In other words, $D_1$ is containing one important information that the farmer is uniquely identified by the name but it also misses one important information that the farmer is engaged in agriculture
\end{itemize}

For evaluation, we used only a simple prompt instructing the LLM to briefly describe the given class in one sentence. For the LLM we used only Mixtral-8x7B. For the RAG we used no-filtering, semantic, and syntactic approach and we used only the \emph{zoological gardens}\footnote{\url{https://github.com/dataspecer/domain-modeling-benchmark/blob/d9e00b65a376dbefe5b605bd680621f9f63f9252/manual\%20evaluation\%20domains/zoological\%20gardens/domain-description-00.txt}} domain description. The evaluation was done only by one human evaluator. For each question we summed up all assigned points and divided the result by the total number of classes from the reference model of the given domain description.

\begin{table}[!h]
    \scriptsize
    \centering
    \setlength{\tabcolsep}{0.5em}
    \begin{tabular}{lcccc}
     \toprule
        RAG approach & Q1 & Q2 & Q3 & Average \\
    \toprule
    
    \addlinespace
         none       & 0.86 & 0.96 & 0.89 & 0.90 \\
    	 semantic   & 0.82 & 0.89 & 0.89 & 0.87 \\
         syntactic  & 0.93 & 0.93 & 0.93 & \textbf{0.93} \\
    \addlinespace
    \bottomrule
    \addlinespace
    \end{tabular}
    \caption{Class descriptions evaluation with Mixtral-8x7B}
    \label{tab:mixtral-classes-descriptions}
\end{table}

Table \ref{tab:mixtral-classes-descriptions} shows the results. As we can see, almost all evaluated class descriptions contained information solely based on the given domain description, provided only the relevant information based on the given class and did not miss any important information. The baseline approach without RAG achieves already a high score which is probably the reason why RAG does not seem to provide any significant improvement for generated class descriptions.


\subsection{Limitations}

The limitations of the evaluation of the generated class descriptions are that for each RAG approach we evaluated only 14 generated class descriptions as the this task needs to be done manually. Also, we used only one domain description that contains a little bit bellow 1000 words.


\section{Evaluation scripts}

All the test data for evaluation were automatically generated by the scripts in our repository except for the generated classes from the ChatGPT-4o since we used the OpenAI's web user interface and copied the output manually. Additionally, all the manually filled in data for generated classes, attributes, and associations were automatically evaluated by a script in our repository\footnote{\url{https://github.com/Dominik7131/Conceptual-Modeling-LLM-Assistant/tree/master/backend/data-processing}}. This script also reports anomalies encountered during the evaluation. For example, each manually filled in domain element that is not automatically found in the reference domain model, is reported.


\section{User-based evaluation of the application}

In addition to the measurements of the performance of the generators, we also evaluated how real domain modelers accept an automated domain modeling assistant. 

We prepared two domain descriptions, a summary of the main features of the prototype, and three 2-minute video tutorials. We instructed real users to model the two domains and then asked them to assign a number between 0 (fully disagree) and 4 (fully agree) to express their agreement with seven different claims about using our prototype tool.
They also classified themselves as teachers, students, modeling experts, database experts, programmers, or managers (multiple options were allowed).
We received responses from 19 users. During the evaluation Mixtral-8x7B was used only with the syntactic RAG approach.

Figure \ref{fig:user-based-evaluation} shows for each user type their average rating. of users the number of responses that picked this type and the average marks received for that type. The last line shows the average of all users.

\begin{figure}[!h]
    %\centering
    \includegraphics[width=1\linewidth]{img/user-based-evaluation.png} \\
    \scriptsize
\raggedright{C1: Modeling with the assistant is better compared to manual modeling. \\
C2: The assistant is intuitive to use.\\
C3: The assistant suggests appropriate classes, attributes, and associations.\\
C4: The assistant suggests appropriate descriptions of classes, attributes, and associations \\
C5: The assistant helped me solve the tasks faster compared to manual modeling.\\
C6: I solved all the tasks using only the assistant, and manual modeling was not needed. \\
C7: The highlighting original text feature helped me to make sure that I completed both tasks}
    \caption{Summary of the user-based evaluation. 0=fully disagree, 4=fully agree.}
    \label{fig:user-based-evaluation}
\end{figure}

As we can see, the users agree that using the automated assistant is better than pure manual modeling (C1) and found it intuitive (C2).
The suggestions provided by the tool are subjectively good but not always appropriate (C3, C4), which is also supported by our measurements presented in the previous part, and manual modeling is still necessary (C6). The highlighting original text feature slightly helped the users to decide if they have modeled the domain descriptions (C7).
Despite these imperfections, users claimed that the prototype made them more productive (C5).
We can find the weakest support in the manager category, but we were able to get only one response in this category, so a further evaluation is necessary.